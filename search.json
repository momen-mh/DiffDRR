[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DiffDRR",
    "section": "",
    "text": "Auto-differentiable DRR rendering and optimization in PyTorch\nDiffDRR is a PyTorch-based digitally reconstructed radiograph (DRR) generator that provides\nMost importantly, DiffDRR implements DRR rendering as a PyTorch module, making it interoperable in deep learning pipelines.",
    "crumbs": [
      "DiffDRR"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "DiffDRR",
    "section": "Install",
    "text": "Install\npip install diffdrr",
    "crumbs": [
      "DiffDRR"
    ]
  },
  {
    "objectID": "index.html#hello-world",
    "href": "index.html#hello-world",
    "title": "DiffDRR",
    "section": "Hello, World!",
    "text": "Hello, World!\nThe following minimal example specifies the geometry of the projectional radiograph imaging system and traces rays through a CT volume:\n\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom diffdrr.drr import DRR\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.visualization import plot_drr\n\n# Read in the volume and get its origin and spacing in world coordinates\nsubject = load_example_ct()\n\n# Initialize the DRR module for generating synthetic X-rays\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr = DRR(\n    subject,     # An object storing the CT volume, origin, and voxel spacing\n    sdd=1020.0,  # Source-to-detector distance (i.e., focal length)\n    height=200,  # Image height (if width is not provided, the generated DRR is square)\n    delx=2.0,    # Pixel spacing (in mm)\n).to(device)\n\n# Set the camera pose with rotations (yaw, pitch, roll) and translations (x, y, z)\nrotations = torch.tensor([[0.0, 0.0, 0.0]], device=device)\ntranslations = torch.tensor([[0.0, 850.0, 0.0]], device=device)\n\n# üì∏ Also note that DiffDRR can take many representations of SO(3) üì∏\n# For example, quaternions, rotation matrix, axis-angle, etc...\nimg = drr(rotations, translations, parameterization=\"euler_angles\", convention=\"ZXY\")\nplot_drr(img, ticks=False)\nplt.show()\n\n\n\n\n\n\n\n\nOn a single NVIDIA RTX 2080 Ti GPU, producing such an image takes\n\n\n\n37.9 ms ¬± 19.6 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\nThe full example is available at introduction.ipynb.",
    "crumbs": [
      "DiffDRR"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "DiffDRR",
    "section": "Usage",
    "text": "Usage\n\nRendering\nThe physics-based pipeline in DiffDRR renders photorealistic X-rays. For example, compare a real X-ray to a synthetic X-ray rendered from a CT of the same patient using DiffDRR (X-rays and CTs from the DeepFluoro dataset):\n\n\n\nDiffDRR rendering from the same camera pose as a real X-ray.\n\n\n\n\n2D/3D Registration\nThe impotus for developing DiffDRR was to solve 2D/3D registration problems with gradient-based optimization. Here, we demonstrate DiffDRR‚Äôs capabilities by generating two DRRs:\n\nA fixed DRR from a set of ground truth parameters\nA moving DRR from randomly initialized parameters\n\nTo align the two images, we use gradient descent to maximize an image similarity metric between the two DRRs. This produces optimization runs like this:\n\n\n\nIterative optimization of moving DRR to a target DRR.\n\n\nThe full example is available at optimizers.ipynb\n\nüÜï Examples on Real-World Data üÜï\nFor examples running DiffDRR on real surgical datasets, check out our latest work, DiffPose:\n\n\n\nRegistering real X-rays to real CT scans.\n\n\nThis work includes a lot of real-world usecases of DiffDRR including\n\nUsing DiffDRR as a layer in a deep learning architecture\nAlignment of real X-rays and rendered DRRs\nAchieving sub-millimeter registration accuracy very quickly\n\n\n\n\nSegmentation\nDiffDRR can project 3D labelmaps into 2D simply using perspective geometry, helping identify particular structures in simulated X-rays (these labels come from the TotalSegmentator v2 dataset):\n\n\n\nProjected segmentation masks.\n\n\n\n\nVolume Reconstruction\nDiffDRR is differentiable with respect to the 3D volume as well as camera poses. Therefore, it could (in theory) be used for volume reconstruction via differentiable rendering. However, this feature has not been robustly tested and is currently under active development (see reconstruction.ipynb)!",
    "crumbs": [
      "DiffDRR"
    ]
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "DiffDRR",
    "section": "Development",
    "text": "Development\nDiffDRR source code, docs, and CI are all built using nbdev. To get set up with nbdev, install the following\nmamba install jupyterlab nbdev -c fastai -c conda-forge \nnbdev_install_quarto  # To build docs\nnbdev_install_hooks   # Make notebooks git-friendly\nRunning nbdev_help will give you the full list of options. The most important ones are\nnbdev_preview  # Render docs locally and inspect in browser\nnbdev_clean    # NECESSARY BEFORE PUSHING\nnbdev_test     # tests notebooks\nnbdev_export   # builds package and builds docs\nFor more details, follow this in-depth tutorial.",
    "crumbs": [
      "DiffDRR"
    ]
  },
  {
    "objectID": "index.html#how-does-diffdrr-work",
    "href": "index.html#how-does-diffdrr-work",
    "title": "DiffDRR",
    "section": "How does DiffDRR work?",
    "text": "How does DiffDRR work?\nDiffDRR reformulates Siddon‚Äôs method,1 the canonical algorithm for calculating the radiologic path of an X-ray through a volume, as a series of vectorized tensor operations. This version of the algorithm is easily implemented in tensor algebra libraries like PyTorch to achieve a fast auto-differentiable DRR generator.",
    "crumbs": [
      "DiffDRR"
    ]
  },
  {
    "objectID": "index.html#citing-diffdrr",
    "href": "index.html#citing-diffdrr",
    "title": "DiffDRR",
    "section": "Citing DiffDRR",
    "text": "Citing DiffDRR\nIf you find DiffDRR useful in your work, please cite our paper (or the freely accessible arXiv version):\n@inproceedings{gopalakrishnanDiffDRR2022,\n    author    = {Gopalakrishnan, Vivek and Golland, Polina},\n    title     = {Fast Auto-Differentiable Digitally Reconstructed Radiographs for Solving Inverse Problems in Intraoperative Imaging},\n    year      = {2022},\n    booktitle = {Clinical Image-based Procedures: 11th International Workshop, CLIP 2022, Held in Conjunction with MICCAI 2022, Singapore, Proceedings},\n    series    = {Lecture Notes in Computer Science},\n    publisher = {Springer},\n    doi       = {https://doi.org/10.1007/978-3-031-23179-7_1},\n}",
    "crumbs": [
      "DiffDRR"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "DiffDRR",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSiddon RL. Fast calculation of the exact radiological path for a three-dimensional CT array. Medical Physics, 2(12):252‚Äì5, 1985.‚Ü©Ô∏é",
    "crumbs": [
      "DiffDRR"
    ]
  },
  {
    "objectID": "api/data.html",
    "href": "api/data.html",
    "title": "data",
    "section": "",
    "text": "CT scans in DiffDRR are stored using the torchio.Subject dataclass. torchio provides a convenient and consistent mechanism for reading volumes from a variety of formats and orientations. We canonicalize all volumes to the RAS+ coordinate space. In addition to reading an input volume, you can also pass the following to diffdrr.data.read when loading a subject: - labelmap : a 3D segmentation of the input volume - labels : a subset of structures from the labelmap that you want to render - orientation : a frame-of-reference change for the C-arm (currently, ‚ÄúAP‚Äù and ‚ÄúPA‚Äù are supported) - bone_attenuation_multiplier : a constant multiplier to the estimated density of bone voxels - fiducials : a tensor of 3D fiducial marks in world coordinates - **kwargs : any additional kwargs can be passed to the torchio.Subject and accessed as a dictionary\n\nsource\n\nload_example_ct\n\n load_example_ct (labels=None, orientation='AP',\n                  bone_attenuation_multiplier=1.0, **kwargs)\n\nLoad an example chest CT for demonstration purposes.\n\nsource\n\n\nread\n\n read (volume:str|pathlib.Path|torchio.data.image.ScalarImage,\n       labelmap:str|pathlib.Path|torchio.data.image.LabelMap=None,\n       labels:int|list=None, orientation:str='AP',\n       bone_attenuation_multiplier:float=1.0, fiducials:torch.Tensor=None,\n       **kwargs)\n\nRead an image volume from a variety of formats, and optionally, any given labelmap for the volume. Converts volume to a RAS+ coordinate system and moves the volume isocenter to the world origin.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvolume\nstr | Path | ScalarImage\n\nCT volume\n\n\nlabelmap\nstr | Path | LabelMap\nNone\nLabelmap for the CT volume\n\n\nlabels\nint | list\nNone\nLabels from the mask of structures to render\n\n\norientation\nstr\nAP\nFrame-of-reference change\n\n\nbone_attenuation_multiplier\nfloat\n1.0\nScalar multiplier on density of high attenuation voxels\n\n\nfiducials\ntorch.Tensor\nNone\n3D fiducials in world coordinates\n\n\nkwargs\n\n\n\n\n\nReturns\nSubject\n\nAny additional information to be stored in the torchio.Subject",
    "crumbs": [
      "api",
      "data"
    ]
  },
  {
    "objectID": "api/utils.html",
    "href": "api/utils.html",
    "title": "utils",
    "section": "",
    "text": "From a calibrated camera‚Äôs intrinsic matrix, calculate the following properties:\n\nFocal length (in units length)\nPrincipal point (in units length)\n\n\nsource\n\n\n\n get_focal_length (intrinsic, delx:float, dely:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nintrinsic\n\nIntrinsic matrix (3 x 3 tensor)\n\n\ndelx\nfloat\nX-direction spacing (in units length)\n\n\ndely\nfloat\nY-direction spacing (in units length)\n\n\nReturns\nfloat\nFocal length (in units length)\n\n\n\n\nsource\n\n\n\n\n get_principal_point (intrinsic, height:int, width:int, delx:float,\n                      dely:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nintrinsic\n\nIntrinsic matrix (3 x 3 tensor)\n\n\nheight\nint\nY-direction length (in units pixels)\n\n\nwidth\nint\nX-direction length (in units pixels)\n\n\ndelx\nfloat\nX-direction spacing (in units length)\n\n\ndely\nfloat\nY-direction spacing (in units length)\n\n\n\n\nsource\n\n\n\n\n parse_intrinsic_matrix (intrinsic, height:int, width:int, delx:float,\n                         dely:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nintrinsic\n\nIntrinsic matrix (3 x 3 tensor)\n\n\nheight\nint\nY-direction length (in units pixels)\n\n\nwidth\nint\nX-direction length (in units pixels)\n\n\ndelx\nfloat\nX-direction spacing (in units length)\n\n\ndely\nfloat\nY-direction spacing (in units length)\n\n\n\n\nsource\n\n\n\n\n make_intrinsic_matrix (sdd:float, delx:float, dely:float, height:int,\n                        width:int, x0:float=0.0, y0:float=0.0)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsdd\nfloat\n\nSource-to-detector distance (in units length)\n\n\ndelx\nfloat\n\nX-direction spacing (in units length / pixel)\n\n\ndely\nfloat\n\nY-direction spacing (in units length / pixel)\n\n\nheight\nint\n\nY-direction length (in units pixels)\n\n\nwidth\nint\n\nX-direction length (in units pixels)\n\n\nx0\nfloat\n0.0\nPrincipal point x-coordinate (in units length)\n\n\ny0\nfloat\n0.0\nPrincipal point y-coordinate (in units length)",
    "crumbs": [
      "api",
      "utils"
    ]
  },
  {
    "objectID": "api/utils.html#intrinsic-matrix-parsing",
    "href": "api/utils.html#intrinsic-matrix-parsing",
    "title": "utils",
    "section": "",
    "text": "From a calibrated camera‚Äôs intrinsic matrix, calculate the following properties:\n\nFocal length (in units length)\nPrincipal point (in units length)\n\n\nsource\n\n\n\n get_focal_length (intrinsic, delx:float, dely:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nintrinsic\n\nIntrinsic matrix (3 x 3 tensor)\n\n\ndelx\nfloat\nX-direction spacing (in units length)\n\n\ndely\nfloat\nY-direction spacing (in units length)\n\n\nReturns\nfloat\nFocal length (in units length)\n\n\n\n\nsource\n\n\n\n\n get_principal_point (intrinsic, height:int, width:int, delx:float,\n                      dely:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nintrinsic\n\nIntrinsic matrix (3 x 3 tensor)\n\n\nheight\nint\nY-direction length (in units pixels)\n\n\nwidth\nint\nX-direction length (in units pixels)\n\n\ndelx\nfloat\nX-direction spacing (in units length)\n\n\ndely\nfloat\nY-direction spacing (in units length)\n\n\n\n\nsource\n\n\n\n\n parse_intrinsic_matrix (intrinsic, height:int, width:int, delx:float,\n                         dely:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nintrinsic\n\nIntrinsic matrix (3 x 3 tensor)\n\n\nheight\nint\nY-direction length (in units pixels)\n\n\nwidth\nint\nX-direction length (in units pixels)\n\n\ndelx\nfloat\nX-direction spacing (in units length)\n\n\ndely\nfloat\nY-direction spacing (in units length)\n\n\n\n\nsource\n\n\n\n\n make_intrinsic_matrix (sdd:float, delx:float, dely:float, height:int,\n                        width:int, x0:float=0.0, y0:float=0.0)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsdd\nfloat\n\nSource-to-detector distance (in units length)\n\n\ndelx\nfloat\n\nX-direction spacing (in units length / pixel)\n\n\ndely\nfloat\n\nY-direction spacing (in units length / pixel)\n\n\nheight\nint\n\nY-direction length (in units pixels)\n\n\nwidth\nint\n\nX-direction length (in units pixels)\n\n\nx0\nfloat\n0.0\nPrincipal point x-coordinate (in units length)\n\n\ny0\nfloat\n0.0\nPrincipal point y-coordinate (in units length)",
    "crumbs": [
      "api",
      "utils"
    ]
  },
  {
    "objectID": "api/drr.html",
    "href": "api/drr.html",
    "title": "DRR",
    "section": "",
    "text": "DRR is a PyTorch module that compues differentiable digitally reconstructed radiographs. The viewing angle for the DRR (known generally in computer graphics as the camera pose) is parameterized by the following parameters:\n\nSDD : source-to-detector distance (i.e., the focal length of the C-arm)\n\\(\\mathbf R \\in \\mathrm{SO}(3)\\) : a rotation\n\\(\\mathbf t \\in \\mathbb R^3\\) : a translation\n\n\n\n\n\n\n\nTip\n\n\n\nDiffDRR can take a rotation parameterized in any of the following forms to move the detector plane:\n\naxis_angle\neuler_angles (note: also need to specify the convention for the Euler angles)\nmatrix\nquaternion\nquaternion_adjugate (Hanson and Hanson, 2022)\nrotation_6d (Zhou et al., 2019)\nrotation_10d (Peretroukhin et al., 2021)`\nse3_log_map\n\n\n\nIf using Euler angles, the parameters are\n\nalpha : Azimuthal angle\nbeta : Polar angle\ngamma : Plane rotation angle\nbx : X-dir translation\nby : Y-dir translation\nbz : Z-dir translation\nconvention : Order of angles (e.g., ZYX)\n\n(bx, by, bz) are translational parameters and (alpha, beta, gamma) are rotational parameters.\n\nsource\n\n\n\n DRR (subject:torchio.data.subject.Subject, sdd:float, height:int,\n      delx:float, width:int|None=None, dely:float|None=None, x0:float=0.0,\n      y0:float=0.0, p_subsample:float|None=None, reshape:bool=True,\n      reverse_x_axis:bool=True, patch_size:int|None=None,\n      renderer:str='siddon', **renderer_kwargs)\n\nPyTorch module that computes differentiable digitally reconstructed radiographs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubject\nSubject\n\nTorchIO wrapper for the CT volume\n\n\nsdd\nfloat\n\nSource-to-detector distance (i.e., the C-arm‚Äôs focal length)\n\n\nheight\nint\n\nHeight of the rendered DRR\n\n\ndelx\nfloat\n\nX-axis pixel size\n\n\nwidth\nint | None\nNone\nWidth of the rendered DRR (default to height)\n\n\ndely\nfloat | None\nNone\nY-axis pixel size (if not provided, set to delx)\n\n\nx0\nfloat\n0.0\nPrincipal point X-offset\n\n\ny0\nfloat\n0.0\nPrincipal point Y-offset\n\n\np_subsample\nfloat | None\nNone\nProportion of pixels to randomly subsample\n\n\nreshape\nbool\nTrue\nReturn DRR with shape (b, 1, h, w)\n\n\nreverse_x_axis\nbool\nTrue\nIf True, obey radiologic convention (e.g., heart on right)\n\n\npatch_size\nint | None\nNone\nRender patches of the DRR in series\n\n\nrenderer\nstr\nsiddon\nRendering backend, either ‚Äúsiddon‚Äù or ‚Äútrilinear‚Äù\n\n\nrenderer_kwargs\n\n\n\n\n\n\nThe forward pass of the DRR module generated DRRs from the input CT volume. The pose parameters (i.e., viewing angles) from which the DRRs are generated are passed to the forward call.\n\nsource\n\n\n\n\n DRR.forward (*args, parameterization:str=None, convention:str=None,\n              calibration:diffdrr.pose.RigidTransform=None,\n              mask_to_channels:bool=False, **kwargs)\n\nGenerate DRR with rotational and translational parameters.\n\nsource\n\n\n\n\n DRR.perspective_projection (pose:diffdrr.pose.RigidTransform,\n                             pts:torch.Tensor)\n\n\nsource\n\n\n\n\n DRR.inverse_projection (pose:diffdrr.pose.RigidTransform,\n                         pts:torch.Tensor)",
    "crumbs": [
      "api",
      "DRR"
    ]
  },
  {
    "objectID": "api/drr.html#drr",
    "href": "api/drr.html#drr",
    "title": "DRR",
    "section": "",
    "text": "DRR is a PyTorch module that compues differentiable digitally reconstructed radiographs. The viewing angle for the DRR (known generally in computer graphics as the camera pose) is parameterized by the following parameters:\n\nSDD : source-to-detector distance (i.e., the focal length of the C-arm)\n\\(\\mathbf R \\in \\mathrm{SO}(3)\\) : a rotation\n\\(\\mathbf t \\in \\mathbb R^3\\) : a translation\n\n\n\n\n\n\n\nTip\n\n\n\nDiffDRR can take a rotation parameterized in any of the following forms to move the detector plane:\n\naxis_angle\neuler_angles (note: also need to specify the convention for the Euler angles)\nmatrix\nquaternion\nquaternion_adjugate (Hanson and Hanson, 2022)\nrotation_6d (Zhou et al., 2019)\nrotation_10d (Peretroukhin et al., 2021)`\nse3_log_map\n\n\n\nIf using Euler angles, the parameters are\n\nalpha : Azimuthal angle\nbeta : Polar angle\ngamma : Plane rotation angle\nbx : X-dir translation\nby : Y-dir translation\nbz : Z-dir translation\nconvention : Order of angles (e.g., ZYX)\n\n(bx, by, bz) are translational parameters and (alpha, beta, gamma) are rotational parameters.\n\nsource\n\n\n\n DRR (subject:torchio.data.subject.Subject, sdd:float, height:int,\n      delx:float, width:int|None=None, dely:float|None=None, x0:float=0.0,\n      y0:float=0.0, p_subsample:float|None=None, reshape:bool=True,\n      reverse_x_axis:bool=True, patch_size:int|None=None,\n      renderer:str='siddon', **renderer_kwargs)\n\nPyTorch module that computes differentiable digitally reconstructed radiographs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubject\nSubject\n\nTorchIO wrapper for the CT volume\n\n\nsdd\nfloat\n\nSource-to-detector distance (i.e., the C-arm‚Äôs focal length)\n\n\nheight\nint\n\nHeight of the rendered DRR\n\n\ndelx\nfloat\n\nX-axis pixel size\n\n\nwidth\nint | None\nNone\nWidth of the rendered DRR (default to height)\n\n\ndely\nfloat | None\nNone\nY-axis pixel size (if not provided, set to delx)\n\n\nx0\nfloat\n0.0\nPrincipal point X-offset\n\n\ny0\nfloat\n0.0\nPrincipal point Y-offset\n\n\np_subsample\nfloat | None\nNone\nProportion of pixels to randomly subsample\n\n\nreshape\nbool\nTrue\nReturn DRR with shape (b, 1, h, w)\n\n\nreverse_x_axis\nbool\nTrue\nIf True, obey radiologic convention (e.g., heart on right)\n\n\npatch_size\nint | None\nNone\nRender patches of the DRR in series\n\n\nrenderer\nstr\nsiddon\nRendering backend, either ‚Äúsiddon‚Äù or ‚Äútrilinear‚Äù\n\n\nrenderer_kwargs\n\n\n\n\n\n\nThe forward pass of the DRR module generated DRRs from the input CT volume. The pose parameters (i.e., viewing angles) from which the DRRs are generated are passed to the forward call.\n\nsource\n\n\n\n\n DRR.forward (*args, parameterization:str=None, convention:str=None,\n              calibration:diffdrr.pose.RigidTransform=None,\n              mask_to_channels:bool=False, **kwargs)\n\nGenerate DRR with rotational and translational parameters.\n\nsource\n\n\n\n\n DRR.perspective_projection (pose:diffdrr.pose.RigidTransform,\n                             pts:torch.Tensor)\n\n\nsource\n\n\n\n\n DRR.inverse_projection (pose:diffdrr.pose.RigidTransform,\n                         pts:torch.Tensor)",
    "crumbs": [
      "api",
      "DRR"
    ]
  },
  {
    "objectID": "api/registration.html",
    "href": "api/registration.html",
    "title": "registration",
    "section": "",
    "text": "The Registration module uses the DRR module to perform differentiable 2D-to-3D registration. Initial guesses for the pose parameters are as stored as nn.Parameters of the module. This allows the pose parameters to be optimized with any PyTorch optimizer. Furthermore, this design choice allows DRR to be used purely as a differentiable renderer.\n\nsource\n\n\n\n Registration (drr:diffdrr.drr.DRR, rotation:torch.Tensor,\n               translation:torch.Tensor, parameterization:str,\n               convention:str=None)\n\nPerform automatic 2D-to-3D registration using differentiable rendering.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndrr\nDRR\n\nPreinitialized DRR module\n\n\nrotation\nTensor\n\nInitial guess for rotations\n\n\ntranslation\nTensor\n\nInitial guess for translations\n\n\nparameterization\nstr\n\nSpecifies the representation of the rotation\n\n\nconvention\nstr\nNone\nIf parameterization is euler_angles, specify convention",
    "crumbs": [
      "api",
      "registration"
    ]
  },
  {
    "objectID": "api/registration.html#registration",
    "href": "api/registration.html#registration",
    "title": "registration",
    "section": "",
    "text": "The Registration module uses the DRR module to perform differentiable 2D-to-3D registration. Initial guesses for the pose parameters are as stored as nn.Parameters of the module. This allows the pose parameters to be optimized with any PyTorch optimizer. Furthermore, this design choice allows DRR to be used purely as a differentiable renderer.\n\nsource\n\n\n\n Registration (drr:diffdrr.drr.DRR, rotation:torch.Tensor,\n               translation:torch.Tensor, parameterization:str,\n               convention:str=None)\n\nPerform automatic 2D-to-3D registration using differentiable rendering.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndrr\nDRR\n\nPreinitialized DRR module\n\n\nrotation\nTensor\n\nInitial guess for rotations\n\n\ntranslation\nTensor\n\nInitial guess for translations\n\n\nparameterization\nstr\n\nSpecifies the representation of the rotation\n\n\nconvention\nstr\nNone\nIf parameterization is euler_angles, specify convention",
    "crumbs": [
      "api",
      "registration"
    ]
  },
  {
    "objectID": "api/registration.html#pose-regressor",
    "href": "api/registration.html#pose-regressor",
    "title": "registration",
    "section": "Pose Regressor",
    "text": "Pose Regressor\nWe perform patient-specific X-ray to CT registration by pre-training an encoder/decoder architecture. The encoder, PoseRegressor, is comprised of two networks:\n\nA pretrained backbone (i.e., convolutional or transformer network) that extracts features from an input X-ray image.\nA set of two linear layers that decodes these features into camera pose parameters (a rotation and a translation).\n\nThe decoder is diffdrr.drr.DRR, which renders a simulated X-ray from the predicted pose parameters. Because our renderer is differentiable, a loss metric on the simulated X-ray and the input X-ray can be backpropogated to the encoder.\n\nsource\n\nPoseRegressor\n\n PoseRegressor (model_name, parameterization, convention=None,\n                pretrained=False, height=256, **kwargs)\n\nA PoseRegressor is comprised of a pretrained backbone model that extracts features from an input X-ray and two linear layers that decode these features into rotational and translational camera pose parameters, respectively.",
    "crumbs": [
      "api",
      "registration"
    ]
  },
  {
    "objectID": "tutorials/optimizers.html",
    "href": "tutorials/optimizers.html",
    "title": "2D/3D registration",
    "section": "",
    "text": "To perform registration with DiffDRR, we do the following:",
    "crumbs": [
      "tutorials",
      "2D/3D registration"
    ]
  },
  {
    "objectID": "tutorials/optimizers.html#generate-a-target-x-ray",
    "href": "tutorials/optimizers.html#generate-a-target-x-ray",
    "title": "2D/3D registration",
    "section": "1. Generate a target X-ray",
    "text": "1. Generate a target X-ray\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\n\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.drr import DRR\nfrom diffdrr.metrics import (\n    MultiscaleNormalizedCrossCorrelation2d,\n    NormalizedCrossCorrelation2d,\n)\nfrom diffdrr.registration import Registration\nfrom diffdrr.visualization import plot_drr\n\n# Make the ground truth X-ray\nSDD = 1020.0\nHEIGHT = 100\nDELX = 4.0\n\nsubject = load_example_ct()\ntrue_params = {\n    \"sdr\": SDD,\n    \"alpha\": 0.0,\n    \"beta\": 0.0,\n    \"gamma\": 0.0,\n    \"bx\": 0.0,\n    \"by\": 850.0,\n    \"bz\": 0.0,\n}\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndrr = DRR(subject, sdd=SDD, height=HEIGHT, delx=DELX).to(device)\nrotations = torch.tensor(\n    [\n        [\n            true_params[\"alpha\"],\n            true_params[\"beta\"],\n            true_params[\"gamma\"],\n        ]\n    ]\n).to(device)\ntranslations = torch.tensor(\n    [\n        [\n            true_params[\"bx\"],\n            true_params[\"by\"],\n            true_params[\"bz\"],\n        ]\n    ]\n).to(device)\nground_truth = drr(\n    rotations,\n    translations,\n    parameterization=\"euler_angles\",\n    convention=\"ZXY\",\n)\n\nplot_drr(ground_truth)\nplt.show()",
    "crumbs": [
      "tutorials",
      "2D/3D registration"
    ]
  },
  {
    "objectID": "tutorials/optimizers.html#initialize-a-moving-drr-from-a-random-pose",
    "href": "tutorials/optimizers.html#initialize-a-moving-drr-from-a-random-pose",
    "title": "2D/3D registration",
    "section": "2. Initialize a moving DRR from a random pose",
    "text": "2. Initialize a moving DRR from a random pose\n\nfrom diffdrr.pose import convert\n\n\ndef pose_from_carm(sid, tx, ty, alpha, beta, gamma):\n    rot = torch.tensor([[alpha, beta, gamma]])\n    xyz = torch.tensor([[tx, sid, ty]])\n    return convert(rot, xyz, parameterization=\"euler_angles\", convention=\"ZXY\")\n\ngt_pose = convert(rotations, translations, parameterization=\"euler_angles\", convention=\"ZXY\")\n\n\n\nCode\n# Make a random DRR\nnp.random.seed(5)\n\n\ndef get_initial_parameters(true_params):\n    alpha = true_params[\"alpha\"] + np.random.uniform(-np.pi / 4, np.pi / 4)\n    beta = true_params[\"beta\"] + np.random.uniform(-np.pi / 4, np.pi / 4)\n    gamma = true_params[\"gamma\"] + np.random.uniform(-np.pi / 4, np.pi / 4)\n    bx = true_params[\"bx\"] + np.random.uniform(-30.0, 30.0)\n    by = true_params[\"by\"] + np.random.uniform(-30.0, 30.0)\n    bz = true_params[\"bz\"] + np.random.uniform(-30.0, 30.0)\n    pose = pose_from_carm(by, bx, bz, alpha, beta, gamma).cuda()\n    rotations, translations = pose.convert(\"euler_angles\", \"ZXY\")\n    return rotations, translations, pose\n\n\nrotations, translations, pose = get_initial_parameters(true_params)\ndrr = DRR(subject, sdd=SDD, height=HEIGHT, delx=DELX).to(device)\nwith torch.no_grad():\n    est = drr(pose)\nplot_drr(est)\nplt.show()\n\nrotations, translations\n\n\n\n\n\n\n\n\n\n(tensor([[-0.4367,  0.5823, -0.4607]], device='cuda:0'),\n tensor([[ 25.1167, 849.3047,   6.7047]], device='cuda:0'))",
    "crumbs": [
      "tutorials",
      "2D/3D registration"
    ]
  },
  {
    "objectID": "tutorials/optimizers.html#measure-the-loss-between-the-target-x-ray-and-moving-drr",
    "href": "tutorials/optimizers.html#measure-the-loss-between-the-target-x-ray-and-moving-drr",
    "title": "2D/3D registration",
    "section": "3. Measure the loss between the target X-ray and moving DRR",
    "text": "3. Measure the loss between the target X-ray and moving DRR\nWe start by measuring the initial loss between the two images.\n\ncriterion = NormalizedCrossCorrelation2d()\ncriterion(ground_truth, est).item()\n\n0.42500585317611694\n\n\nIf the negative normalized cross-correlation is greater than 0.999, we say the target and moving DRR have converged.",
    "crumbs": [
      "tutorials",
      "2D/3D registration"
    ]
  },
  {
    "objectID": "tutorials/optimizers.html#backpropogate-the-loss-to-the-moving-drr-parameters",
    "href": "tutorials/optimizers.html#backpropogate-the-loss-to-the-moving-drr-parameters",
    "title": "2D/3D registration",
    "section": "4. Backpropogate the loss to the moving DRR parameters",
    "text": "4. Backpropogate the loss to the moving DRR parameters\nWe also use this example to show how different optimizers affect the outcome of registration. The parameters we tweak are\n\nlr_rotations: learning rate for rotation parameters\nlr_translations: learning rate for translation parameters\nmomentum: momentum for stochastic gradient descent\ndampening: dampening for stochastic gradient descent\n\n\ndef optimize(\n    reg: Registration,\n    ground_truth,\n    lr_rotations=5e-2,\n    lr_translations=1e2,\n    momentum=0,\n    dampening=0,\n    n_itrs=250,\n    optimizer=\"sgd\",  # 'sgd' or `adam`\n):\n    if optimizer == \"sgd\":\n        optimizer = torch.optim.SGD(\n            [\n                {\"params\": [reg._rotation], \"lr\": lr_rotations},\n                {\"params\": [reg._translation], \"lr\": lr_translations},\n            ],\n            momentum=momentum,\n            dampening=dampening,\n            maximize=True,\n        )\n    else:\n        optimizer = torch.optim.Adam(\n            [\n                {\"params\": [reg._rotation], \"lr\": lr_rotations},\n                {\"params\": [reg._translation], \"lr\": lr_translations},\n            ],\n            maximize=True,\n        )\n\n    params = []\n    losses = []\n    for itr in tqdm(range(n_itrs), ncols=50):\n        # Save the current set of parameters\n        alpha, beta, gamma = reg.rotation.squeeze().tolist()\n        bx, by, bz = reg.translation.squeeze().tolist()\n        params.append([i for i in [alpha, beta, gamma, bx, by, bz]])\n\n        # Run the optimization loop\n        optimizer.zero_grad()\n        estimate = reg()\n        loss = criterion(ground_truth, estimate)\n        loss.backward(retain_graph=True)\n        optimizer.step()\n        losses.append(loss.item())\n\n        if loss &gt; 0.999:\n            tqdm.write(f\"Converged in {itr} iterations\")\n            break\n\n    plt.show()\n    df = pd.DataFrame(params, columns=[\"alpha\", \"beta\", \"gamma\", \"bx\", \"by\", \"bz\"])\n    df[\"loss\"] = losses\n    return df",
    "crumbs": [
      "tutorials",
      "2D/3D registration"
    ]
  },
  {
    "objectID": "tutorials/optimizers.html#run-the-optimization-algorithm",
    "href": "tutorials/optimizers.html#run-the-optimization-algorithm",
    "title": "2D/3D registration",
    "section": "Run the optimization algorithm",
    "text": "Run the optimization algorithm\n\n# Base SGD\ndrr = DRR(subject, sdd=SDD, height=HEIGHT, delx=DELX).to(device)\nreg = Registration(\n    drr,\n    rotations.clone(),\n    translations.clone(),\n    parameterization=\"euler_angles\",\n    convention=\"ZXY\",\n)\nparams_base = optimize(reg, ground_truth)\nprint(params_base[\"loss\"].iloc[-1])\ndel drr\n\n# SGD + momentum\ndrr = DRR(subject, sdd=SDD, height=HEIGHT, delx=DELX).to(device)\nreg = Registration(\n    drr,\n    rotations.clone(),\n    translations.clone(),\n    parameterization=\"euler_angles\",\n    convention=\"ZXY\",\n)\nparams_momentum = optimize(reg, ground_truth, momentum=1e-1)\nprint(params_momentum[\"loss\"].iloc[-1])\ndel drr\n\n# SGD + momentum + dampening\ndrr = DRR(subject, sdd=SDD, height=HEIGHT, delx=DELX).to(device)\nreg = Registration(\n    drr,\n    rotations.clone(),\n    translations.clone(),\n    parameterization=\"euler_angles\",\n    convention=\"ZXY\",\n)\nparams_momentum_dampen = optimize(reg, ground_truth, momentum=1e-1, dampening=1e-2)\nprint(params_momentum_dampen[\"loss\"].iloc[-1])\ndel drr\n\n# Adam\ndrr = DRR(subject, sdd=SDD, height=HEIGHT, delx=DELX).to(device)\nreg = Registration(\n    drr,\n    rotations.clone(),\n    translations.clone(),\n    parameterization=\"euler_angles\",\n    convention=\"ZXY\",\n)\nparams_adam = optimize(reg, ground_truth, 1e-1, 1e1, optimizer=\"adam\")\nprint(params_adam[\"loss\"].iloc[-1])\ndel drr\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 133/250 [00:03&lt;00:03, 37.92it/s]\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 141/250 [00:03&lt;00:02, 42.01it/s]\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 150/250 [00:03&lt;00:02, 42.41it/s]\n 23%|‚ñà‚ñà‚ñä         | 58/250 [00:01&lt;00:04, 39.37it/s]\n\n\nConverged in 133 iterations\n0.9990939497947693\nConverged in 141 iterations\n0.999099612236023\nConverged in 150 iterations\n0.9990929365158081\nConverged in 58 iterations\n0.9992737174034119",
    "crumbs": [
      "tutorials",
      "2D/3D registration"
    ]
  },
  {
    "objectID": "tutorials/optimizers.html#visualize-the-parameter-updates",
    "href": "tutorials/optimizers.html#visualize-the-parameter-updates",
    "title": "2D/3D registration",
    "section": "Visualize the parameter updates",
    "text": "Visualize the parameter updates\nNote that differences that between different optimization algorithms can be seen in the motion in the DRRs!\n\n\nCode\nfrom base64 import b64encode\n\nfrom IPython.display import HTML, display\n\nfrom diffdrr.visualization import animate\n\n\ndef animate_in_browser(df):\n    n = MAX_LENGTH - len(df)\n    df = pd.concat([df, df.iloc[[-1] * n]])\n\n    out = animate(\n        \"&lt;bytes&gt;\",\n        df,\n        drr,\n        ground_truth=ground_truth,\n        verbose=True,\n        device=device,\n        extension=\".webp\",\n        duration=30,\n        parameterization=\"euler_angles\",\n        convention=\"ZXY\",\n    )\n    display(HTML(f\"\"\"&lt;img src='{\"data:img/gif;base64,\" + b64encode(out).decode()}'&gt;\"\"\"))\n\n\nMAX_LENGTH = max(\n    map(\n        len,\n        [\n            params_base,\n            params_momentum,\n            params_momentum_dampen,\n            params_adam,\n        ],\n    )\n)\ndrr = DRR(subject, sdd=SDD, height=HEIGHT, delx=DELX).to(device)\n\n\n\nanimate_in_browser(params_base)\n\nPrecomputing DRRs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:27&lt;00:00,  5.56it/s]\n\n\n\n\n\n\nanimate_in_browser(params_momentum)\n\nPrecomputing DRRs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:27&lt;00:00,  5.46it/s]\n\n\n\n\n\n\nanimate_in_browser(params_momentum_dampen)\n\nPrecomputing DRRs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:27&lt;00:00,  5.54it/s]\n\n\n\n\n\n\nanimate_in_browser(params_adam)\n\nPrecomputing DRRs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:27&lt;00:00,  5.43it/s]",
    "crumbs": [
      "tutorials",
      "2D/3D registration"
    ]
  },
  {
    "objectID": "tutorials/optimizers.html#visualize-the-optimization-trajectories",
    "href": "tutorials/optimizers.html#visualize-the-optimization-trajectories",
    "title": "2D/3D registration",
    "section": "Visualize the optimization trajectories",
    "text": "Visualize the optimization trajectories\n\nimport pyvista\n\nfrom diffdrr.visualization import drr_to_mesh, img_to_mesh\n\n\ndef df_to_mesh(drr, df):\n    pts = []\n    for idx in tqdm(range(len(df))):\n        rot = torch.tensor(df.iloc[idx][[\"alpha\", \"beta\", \"gamma\"]].tolist())\n        xyz = torch.tensor(df.iloc[idx][[\"bx\", \"by\", \"bz\"]].tolist())\n        pose = convert(\n            rot.unsqueeze(0),\n            xyz.unsqueeze(0),\n            parameterization=\"euler_angles\",\n            convention=\"ZXY\",\n        ).cuda()\n        with torch.no_grad():\n            source, _ = drr.detector(pose, None)\n        pts.append(source.squeeze().cpu().tolist())\n    return *img_to_mesh(drr, pose), lines_from_points(np.array(pts))\n\n\ndef lines_from_points(points):\n    \"\"\"Given an array of points, make a line set\"\"\"\n    poly = pyvista.PolyData()\n    poly.points = points\n    cells = np.full((len(points) - 1, 3), 2, dtype=np.int_)\n    cells[:, 1] = np.arange(0, len(points) - 1, dtype=np.int_)\n    cells[:, 2] = np.arange(1, len(points), dtype=np.int_)\n    poly.lines = cells\n    return poly.tube(radius=3)\n\n\nplotter = pyvista.Plotter()\nct = drr_to_mesh(drr.subject, \"surface_nets\", 225, verbose=False)\nplotter.add_mesh(ct)\n\ncamera, detector, texture, principal_ray, points = df_to_mesh(drr, params_base)\nplotter.add_mesh(camera, show_edges=True, line_width=1.5)\nplotter.add_mesh(principal_ray, color=\"lime\", line_width=3)\nplotter.add_mesh(detector, texture=texture)\nplotter.add_mesh(points, color=\"lime\")\n\ncamera, detector, texture, principal_ray, points = df_to_mesh(drr, params_momentum)\nplotter.add_mesh(camera, show_edges=True, line_width=1.5)\nplotter.add_mesh(principal_ray, color=\"deeppink\", line_width=3)\nplotter.add_mesh(detector, texture=texture)\nplotter.add_mesh(points, color=\"deeppink\")\n\ncamera, detector, texture, principal_ray, points = df_to_mesh(\n    drr, params_momentum_dampen\n)\nplotter.add_mesh(camera, show_edges=True, line_width=1.5)\nplotter.add_mesh(principal_ray, color=\"darkorange\", line_width=3)\nplotter.add_mesh(detector, texture=texture)\nplotter.add_mesh(points, color=\"darkorange\")\n\ncamera, detector, texture, principal_ray, points = df_to_mesh(drr, params_adam)\nplotter.add_mesh(camera, show_edges=True, line_width=1.5)\nplotter.add_mesh(principal_ray, color=\"cyan\", line_width=3)\nplotter.add_mesh(detector, texture=texture)\nplotter.add_mesh(points, color=\"cyan\")\n\ncamera, detector, texture, principal_ray = img_to_mesh(drr, gt_pose)\nplotter.add_mesh(camera, show_edges=True, line_width=1.5)\nplotter.add_mesh(principal_ray, color=\"black\", line_width=3)\nplotter.add_mesh(detector, texture=texture)\n\n# Render the plot\nplotter.add_axes()\nplotter.add_bounding_box()\n\nplotter.export_html(\"registration.html\")\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 134/134 [00:00&lt;00:00, 566.04it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142/142 [00:00&lt;00:00, 618.04it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [00:00&lt;00:00, 617.54it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [00:00&lt;00:00, 626.38it/s]\n\n\n\nfrom IPython.display import IFrame\n\nIFrame(\"registration.html\", height=500, width=749)",
    "crumbs": [
      "tutorials",
      "2D/3D registration"
    ]
  },
  {
    "objectID": "tutorials/timing.html",
    "href": "tutorials/timing.html",
    "title": "Timing versus DRR size",
    "section": "",
    "text": "import numpy as np\nimport torch\n\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.drr import DRR\nfrom diffdrr.visualization import plot_drr\n\n\n# Read in the volume\nsubject = load_example_ct()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Get parameters for the detector\nrotations = torch.tensor([[0.0, 0.0, 0.0]], device=device)\ntranslations = torch.tensor([[0.0, 850.0, 0.0]], device=device)\n\n\nheight = 100\n\ndrr = DRR(subject, sdd=1020, height=height, delx=2.0).to(device)\n\ndel drr\n\n10.1 ms ¬± 1.47 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\nheight = 200\n\ndrr = DRR(subject, sdd=1020, height=height, delx=2.0).to(device)\n\ndel drr\n\n38.1 ms ¬± 47.1 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\n\nheight = 300\n\ndrr = DRR(subject, sdd=1020, height=height, delx=2.0).to(device)\n\ndel drr\n\n84.6 ms ¬± 94 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\n\nMemory constraints\nUp until this point, we could compute every ray in the DRR in one go on the GPU. However, as the DRRs get bigger, we will quickly run out of memory. For example, on a 12 GB GPU, computing a 600 by 600 DRR will raise a CUDA memory error.\n\n\n\n\n\n\nTip\n\n\n\nTo render DRRs whose computation won‚Äôt fit in memory, we can compute patches of the DRR at a time. Pass patch_size to the DRR module to specify the size of the patch. Note the patch size must evenly tile (height, width).\n\n\n\nheight = 600\npatch_size = 150\n\ndrr = DRR(subject, sdd=1020, height=height, delx=2.0, patch_size=patch_size).to(device)\n\ndel drr\n\n210 ms ¬± 379 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\nheight = 750\npatch_size = 150\n\ndrr = DRR(subject, sdd=1020, height=height, delx=2.0, patch_size=patch_size).to(device)\n\ndel drr\n\n295 ms ¬± 293 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\nheight = 1000\npatch_size = 250\n\ndrr = DRR(subject, sdd=1020, height=height, delx=2.0, patch_size=patch_size).to(device)\n\ndel drr\n\n458 ms ¬± 612 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\nheight = 1500\npatch_size = 250\n\ndrr = DRR(subject, sdd=1020, height=height, delx=2.0, patch_size=patch_size).to(device)\n\ndel drr\n\n906 ms ¬± 232 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\nWith patch_size, the only limitation is storage in memory, not computation.",
    "crumbs": [
      "tutorials",
      "Timing versus DRR size"
    ]
  },
  {
    "objectID": "tutorials/geometry.html",
    "href": "tutorials/geometry.html",
    "title": "3D geometry in DiffDRR",
    "section": "",
    "text": "This is a tutorial on the 3D geometry used in DiffDRR. It includes:",
    "crumbs": [
      "tutorials",
      "3D geometry in `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/geometry.html#d-visualization-with-pyvista",
    "href": "tutorials/geometry.html#d-visualization-with-pyvista",
    "title": "3D geometry in DiffDRR",
    "section": "3D visualization with PyVista",
    "text": "3D visualization with PyVista\nVisualizing the 3D geometry of the X-ray detector in DiffDRR can be a helpful sanity check and is useful for debugging. We enable visualization of the DiffDRR setup using PyVista. The dependencies are pyvista, trame, and vtk.\nThe 3D visualization functions in DiffDRR perform the following steps:\n\nExtract a mesh from your CT volume\nPlot a pyramid frustum to visualize the camera pose\nPlot the detector plane with the DRR embedded as a texture\nDraw the principal ray from the X-ray source to the detector plane\n\nWe currently support the following backends for extracting meshes from CT scans:\n\nMarchingCubes\nSurfaceNets\n\nAs of DiffDRR v0.4.0, we also support the rendering of 3D labelmaps (e.g., segmentations of CT scans with TotalSegmentator).\n\n\n\n\n\n\nTip\n\n\n\nTo use surface_nets to extract a mesh, ensure you have installed pyvista&gt;=0.43 and vtk&gt;=9.3. Otherwise, you can use marching_cubes, which is slower and produces meshes with holes.\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pyvista\nimport torch\n\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.drr import DRR\nfrom diffdrr.pose import convert\nfrom diffdrr.visualization import drr_to_mesh, img_to_mesh, labelmap_to_mesh, plot_drr\n\npyvista.start_xvfb()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n# Read in the CT volume\nsubject = load_example_ct()\n\n# Make a mesh from the CT volume\nct = drr_to_mesh(subject, \"surface_nets\", threshold=225, verbose=True)\n\nPerforming Labeled Surface Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[00:01&lt;00:00]\nFinding and Labeling Connected Regions.: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[00:01&lt;00:00]\nSmoothing Mesh using Taubin Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[00:05&lt;00:00]\nFilling Holes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[00:00&lt;00:00]\nCleaning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[00:00&lt;00:00]\n\n\n\n# Initialize the DRR module for generating synthetic X-rays\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr = DRR(subject, sdd=1020.0, height=200, delx=2.0).to(device)\n\n# Make a pose\nrot = torch.tensor([[45.0, 30.0, 0.0]], device=device) / 180 * torch.pi\nxyz = torch.tensor([[0.0, 800.0, 0.0]], device=device)\npose = convert(rot, xyz, parameterization=\"euler_angles\", convention=\"ZXY\")\nplot_drr(drr(pose))\nplt.show()\n\n\n\n\n\n\n\n\n\n# Make a mesh from the camera and detector plane\ncamera, detector, texture, principal_ray = img_to_mesh(drr, pose)\n\n# Make the plot\nplotter = pyvista.Plotter()\nplotter.add_mesh(ct)\nplotter.add_mesh(camera, show_edges=True, line_width=1.5)\nplotter.add_mesh(principal_ray, color=\"lime\", line_width=3)\nplotter.add_mesh(detector, texture=texture)\n\n# Render the plot\nplotter.add_axes()\nplotter.add_bounding_box()\nplotter.show_bounds(grid=\"front\", location=\"outer\", all_edges=True)\n\n# plotter.show()  # If running Jupyter locally\n# plotter.show(jupyter_backend=\"server\")  # If running Jupyter remotely\nplotter.export_html(\"render.html\")\n\n\nfrom IPython.display import IFrame\n\nIFrame(\"render.html\", height=500, width=749)\n\n\n        \n        \n\n\n\nRendering labelmaps\nThe SurfaceNets algorithm was actually originally designed for the visualization of 3D labelmaps. Running it on segmentation masks produced by TotalSegmentator produces detailed renderings.\n\nfrom matplotlib.colors import ListedColormap\n\nmask = labelmap_to_mesh(drr.subject)\ncmap = ListedColormap(\n    [\n        \"#66c2a5\",\n        \"#fc8d62\",\n        \"#8da0cb\",\n        \"#e78ac3\",\n        \"#a6d854\",\n        \"#ffd92f\",\n        \"#e5c494\",\n        \"#b3b3b3\",\n    ]\n    * 15\n)\n\nplotter = pyvista.Plotter()\nplotter.add_mesh(mask, cmap=cmap)\nplotter.add_mesh(camera, show_edges=True, line_width=1.5)\nplotter.add_mesh(principal_ray, color=\"lime\", line_width=3)\nplotter.add_mesh(detector, texture=texture)\nplotter.add_axes()\nplotter.add_bounding_box()\nplotter.remove_scalar_bar()\nplotter.export_html(\"mask.html\")\n\nPerforming Labeled Surface Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[00:02&lt;00:00]\nSmoothing Mesh using Taubin Smoothing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[00:07&lt;00:00]\nCleaning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà[00:00&lt;00:00]\n\n\n\nIFrame(\"mask.html\", height=500, width=749)",
    "crumbs": [
      "tutorials",
      "3D geometry in `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/geometry.html#projective-geometry",
    "href": "tutorials/geometry.html#projective-geometry",
    "title": "3D geometry in DiffDRR",
    "section": "Projective geometry",
    "text": "Projective geometry\nX-ray imaging systems can be accurately modelled as pinhole cameras. What follows is a brief description of the projective geometry underlying pinhole cameras, as it applies to DiffDRR. A more comprehensive overview can be found in Hartley and Zisserman (Chapter 5) or numerous online resources.\n\nIntrinsic parameters\nIn DiffDRR, the intrinsic parameters are\n\nsdd : the source to detector distance (i.e., the C-arm‚Äôs focal length)\ndelx : the x-direction pixel spacing (in mm)\ndely : the y-direction pixel spacing (in mm)\nx0 : the principal point offset in the x-direction\ny0 : the principal point offset in the y-direction\n\nThese direclty form the intrinsic matrix (with physical units) of a pinhole camera. For most imaging systems, these parameters are directly found in the DICOM.\n\n\nExtrinsic parameters\nThe extrinsic parameters comprise an affine transform, written as \\(\\mathbf T \\in \\mathbf{SE}(3)\\), which is an element of a particular manifold of \\(4 \\times 4\\) matrices. There are many ways to parameterize affine transforms. For particulars on how this is implemented in DiffDRR, see the module diffdrr.pose.\nThe simplest way of parameterizing an affine transform is with a translation and a rotation written in Euler angles. In fact, this is how camera poses are often stored in DICOM.",
    "crumbs": [
      "tutorials",
      "3D geometry in `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/geometry.html#relation-to-c-arms",
    "href": "tutorials/geometry.html#relation-to-c-arms",
    "title": "3D geometry in DiffDRR",
    "section": "Relation to C-arms",
    "text": "Relation to C-arms\nFor an example, consider the pose rendered above:\n\nrot = torch.tensor([[45.0, 30.0, 0.0]], device=device) / 180 * torch.pi\nxyz = torch.tensor([[0.0, 800.0, 0.0]], device=device)\npose = convert(rot, xyz, parameterization=\"euler_angles\", convention=\"ZXY\")\n\nThis corresponds to the following sequence of transforms:\n\nA translation of +800 along the y-axis\nA rotation by 45 degrees about the z-axis\nA rotation by 30 degress about the x-axis\n\nwhich produces the oblique view visualized above.\n\nWhat if the C-arm is behind the patient?\nMany medical disciplies have C-arms where the source is positioned behind the patient (e.g., X-ray angiography). In these cases, the CT can be loaded with the PA orientation:\n\nsubject = load_example_ct(orientation=\"PA\")\n\nThis will produce the desired behavior. The only difference is that the y parameter in rot needs to be negated.\n\n\nOn representations of rotations\nThere are many ways to represent 3D rotations, and Euler angles are but one option. In general, Euler angles are the most human-understandable parameterization. However, they‚Äôre not the best for optimization problems. For solving problems like 2D/3D registration, higher-dimensional representations have emperically better performance.",
    "crumbs": [
      "tutorials",
      "3D geometry in `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/introduction.html",
    "href": "tutorials/introduction.html",
    "title": "How to use DiffDRR",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.drr import DRR\nfrom diffdrr.visualization import plot_drr\n\nsns.set_context(\"talk\")",
    "crumbs": [
      "tutorials",
      "How to use `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#rendering-drrs",
    "href": "tutorials/introduction.html#rendering-drrs",
    "title": "How to use DiffDRR",
    "section": "Rendering DRRs",
    "text": "Rendering DRRs\nDiffDRR is implemented as a custom PyTorch module.\nAll raytracing operations have been formulated in a vectorized function, enabling use of PyTorch‚Äôs GPU support and autograd. This also means that X-ray priojection is interoperable as a layer in deep learning frameworks.\n\n\n\n\n\n\nTip\n\n\n\nRotations can be parameterized with numerous conventions (not just Euler angles). See diffdrr.DRR for more details.\n\n\n\n# Read in the volume and get its origin and spacing in world coordinates\nsubject = load_example_ct(bone_attenuation_multiplier=1.0)\n\n# Initialize the DRR module for generating synthetic X-rays\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr = DRR(\n    subject,  # A torchio.Subject object storing the CT volume, origin, and voxel spacing\n    sdd=1020,  # Source-to-detector distance (i.e., the C-arm's focal length)\n    height=200,  # Height of the DRR (if width is not seperately provided, the generated image is square)\n    delx=2.0,  # Pixel spacing (in mm)\n).to(device)\n\n# Specify the C-arm pose with a rotation (yaw, pitch, roll) and orientation (x, y, z)\nrot = torch.tensor([[0.0, 0.0, 0.0]], device=device)\nxyz = torch.tensor([[0.0, 850.0, 0.0]], device=device)\nimg = drr(rot, xyz, parameterization=\"euler_angles\", convention=\"ZXY\")\nplot_drr(img, ticks=False)\nplt.show()\n\n\n\n\n\n\n\n\nWe demonstrate the speed of DiffDRR by timing repeated DRR synthesis. Timing results are on a single NVIDIA RTX 2080 Ti GPU.\n\n\n\n38 ms ¬± 63.6 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)",
    "crumbs": [
      "tutorials",
      "How to use `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#rendering-multiple-drrs-at-once",
    "href": "tutorials/introduction.html#rendering-multiple-drrs-at-once",
    "title": "How to use DiffDRR",
    "section": "Rendering multiple DRRs at once",
    "text": "Rendering multiple DRRs at once\nThe rotations tensor is expected to be of the size B D, where D is the number of components needed to represent the rotation (e.g., 3 for Euler angles, 4 for quaternions, etc.). The translations tensor expected to be of the size B D.\n\nrot = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, torch.pi]], device=device)\nxyz = torch.tensor([[0.0, 850.0, 0.0], [0.0, 850.0, 0.0]], device=device)\nimg = drr(rot, xyz, parameterization=\"euler_angles\", convention=\"ZXY\")\nplot_drr(img, ticks=False)\nplt.show()\n\n\n\n\n\n\n\n\nNote that rendered DRRs have shape B C H W where - B is the number of camera poses passed to the renderer - C is the number of channels in the rendered images - H is the image height, specified in the constructor of the diffdrr.drr.DRR object - W is the image width, which defaults to the height if not otherwise specified\nTypically, C = 1. However, we can have more channels if rendering individual anatomical structures (see the next section).\n\nimg.shape\n\ntorch.Size([2, 1, 200, 200])",
    "crumbs": [
      "tutorials",
      "How to use `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#rendering-individual-structures-in-separate-channels",
    "href": "tutorials/introduction.html#rendering-individual-structures-in-separate-channels",
    "title": "How to use DiffDRR",
    "section": "Rendering individual structures in separate channels",
    "text": "Rendering individual structures in separate channels\nIf the subject passed to diffdrr.drr.DRR also has a mask attribute (a torchio.LabelMap), we can use this 3D segmentation map to render individual structures in the DRR.\n\nMethod 1\nThe first way to do this is to set mask_to_channels=True in DRR.forward, which will create a new channel for every structure.\n\nfrom diffdrr.pose import convert\n\n# Note that you also have the option to directly pass poses in SE(3) to the renderer\nrot = torch.tensor([[0.0, 0.0, 0.0]], device=device)\nxyz = torch.tensor([[0.0, 850.0, 0.0]], device=device)\npose = convert(rot, xyz, parameterization=\"euler_angles\", convention=\"ZXY\")\n\nimg = drr(pose, mask_to_channels=True)\n\nWe used TotalSegmentator v2 to automatically segment the example CT. This dataset has 118 classes. Therefore, the output image has C = 119 (the zero-th channel is a rendering of the background).\n\nimg.shape\n\ntorch.Size([1, 119, 200, 200])\n\n\nWe incur a small amount of additional overhead to partition these channels during rendering:\n\n\n\n54.1 ms ¬± 534 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\nWe can also visualize all of these channels superimposed on the DRR. Note that summing over the channel dimension recapitulates the original DRR.\n\n\nCode\nfrom diffdrr.visualization import plot_mask\n\n# Relabel classes in the TotalSegmentator dataset\ngroups = {\n    \"skeleton\": \"Appendicular Skeleton\",\n    \"ribs\": \"Ribs\",\n    \"vertebrae\": \"Vertebrae\",\n    \"cardiac\": \"Cardiovasculature\",\n    \"organs\": \"Organs\",\n    \"muscles\": \"Muscles\",\n}\n\n# Plot the segmentation masks\nfig, axs = plt.subplots(\n    nrows=2,\n    ncols=4,\n    figsize=(14, 7.75),\n    tight_layout=True,\n    dpi=300,\n)\n\nim = img.sum(dim=1, keepdim=True)\nplot_drr(im, axs=axs[0, 0], ticks=False, title=\"DRR\")\nplot_drr(im, axs=axs[1, 0], ticks=False, title=\"All Segmentations\")\n\nfor (group, title), ax in zip(groups.items(), axs[:, 1:].flatten()):\n    jdxs = subject.structures.query(f\"group == '{group}'\")[\"id\"].tolist()\n    im = img[:, jdxs]\n    plot_drr(im.sum(dim=1, keepdim=True), title=title, axs=ax, ticks=False)\n    masks = plot_mask(im, axs=ax, return_masks=True)\n    for jdx in range(masks.shape[1]):\n        axs[1, 0].imshow(masks[0, jdx], alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMethod 2\nIf we only care about a subset of the structures, we can instead partition the 3D CT prior to rendering. Note that this method is compatible with different rendering backends.\n\n# Only load the bones in the CT (and the costal cartilage, but it looks weird without it)\nstructures = [\"skeleton\", \"ribs\", \"vertebrae\"]\nlabels = subject.structures.query(f\"group in {structures}\")[\"id\"].tolist()\nsubject = load_example_ct(labels=labels)\ndrr = DRR(subject, sdd=1020, height=200, delx=2.0).to(device)\n\nimg = drr(pose)\nplot_drr(img, ticks=False)\nplt.show()\n\n\n\n\n\n\n\n\nBecause we are rendering all structures at once, we don‚Äôt incur additional overhead.\n\n\n\n38.1 ms ¬± 8.47 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)",
    "crumbs": [
      "tutorials",
      "How to use `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#changing-the-appearance-of-the-rendered-drrs",
    "href": "tutorials/introduction.html#changing-the-appearance-of-the-rendered-drrs",
    "title": "How to use DiffDRR",
    "section": "Changing the appearance of the rendered DRRs",
    "text": "Changing the appearance of the rendered DRRs\nFollowing the implementation of DeepDRR, we threshold CTs according to Hounsfield units:\n\nair : HU ‚â§ -800\nsoft tissue : -800 &lt; HU ‚â§ 350\nbone : 350 &lt; HU\n\nIncreasing the bone_attenuation_multiplier upweights the density of voxels thresholded as bone. That is,\n\nbone_attenuation_multiplier = 0 completely removes bones\nbone_attenuation_multiplier &gt; 1 increases the contrast of bones relative to soft tissue\n\n\nimgs = []\nbone_attenuation_multipliers = [0.0, 1.0, 2.5, 5.0]\nfor bone_attenuation_multiplier in bone_attenuation_multipliers:\n    subject = load_example_ct(bone_attenuation_multiplier=bone_attenuation_multiplier)\n    drr = DRR(subject, sdd=1020.0, height=200, delx=2.0).to(device)\n    imgs.append(drr(pose))\n\nfig, axs = plt.subplots(1, 4, figsize=(14, 7), dpi=300, tight_layout=True)\nplot_drr(torch.concat(imgs), ticks=False, title=bone_attenuation_multipliers, axs=axs)\nplt.show()",
    "crumbs": [
      "tutorials",
      "How to use `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#rendering-sparse-drrs",
    "href": "tutorials/introduction.html#rendering-sparse-drrs",
    "title": "How to use DiffDRR",
    "section": "Rendering sparse DRRs",
    "text": "Rendering sparse DRRs\nYou can also render random sparse subsets of the pixels in a DRR.\n\n\n\n\n\n\nTip\n\n\n\nSparse DRR rendering can be useful in registration and reconstruction tasks when coupled with a pixel-wise loss, such as MSE.\n\n\n\n# Make the DRR with 10% of the pixels\nsubject = load_example_ct()\ndrr = DRR(\n    subject,\n    sdd=1020,\n    height=200,\n    delx=2.0,\n    p_subsample=0.1,  # Set the proportion of pixels that should be rendered\n    reshape=True,  # Map rendered pixels back to their location in true space - useful for plotting, but can be disabled if using MSE as a loss function\n).to(device)\n\n# Make the DRR\nimg = drr(pose)\nplot_drr(img, ticks=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n6.04 ms ¬± 12.2 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)",
    "crumbs": [
      "tutorials",
      "How to use `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/introduction.html#using-different-rendering-backends",
    "href": "tutorials/introduction.html#using-different-rendering-backends",
    "title": "How to use DiffDRR",
    "section": "Using different rendering backends",
    "text": "Using different rendering backends\nDiffDRR can also render synthetic X-rays using trilinear interpolation instead of Siddon‚Äôs method. The key argument to pay attention to is n_points, which controls how many points are sampled along each ray for interpolation. Higher values make more realistic images, at the cost of higher rendering time.\n\ndrr = DRR(\n    subject,\n    sdd=1020,\n    height=200,\n    delx=2.0,\n    renderer=\"trilinear\",  # Set the rendering backend to trilinear\n).to(device)\n\nimgs = []\nn_points = [100, 250, 500, 1000]\nfor n in n_points:\n    img = drr(pose, n_points=n)\n    imgs.append(img)\n\nfig, axs = plt.subplots(1, 4, figsize=(14, 7), dpi=300, tight_layout=True)\nimg = torch.concat(imgs)\naxs = plot_drr(img, ticks=False, title=[f\"n_points={n}\" for n in n_points], axs=axs)\nplt.show()",
    "crumbs": [
      "tutorials",
      "How to use `DiffDRR`"
    ]
  },
  {
    "objectID": "tutorials/trilinear.html",
    "href": "tutorials/trilinear.html",
    "title": "Trilinear rendering",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport torch\nfrom IPython.core.magics.execution import _format_time\n\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.drr import DRR\nfrom diffdrr.pose import convert\nfrom diffdrr.visualization import plot_drr\nsubject = load_example_ct()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Set the camera pose with rotations (yaw, pitch, roll) and translations (x, y, z)\nrotations = torch.tensor([[0.0, 0.0, 0.0]], device=device)\ntranslations = torch.tensor([[0.0, 850.0, 0.0]], device=device)\n\npose = convert(\n    rotations,\n    translations,\n    parameterization=\"euler_angles\",\n    convention=\"ZXY\",\n)"
  },
  {
    "objectID": "tutorials/trilinear.html#siddons-method",
    "href": "tutorials/trilinear.html#siddons-method",
    "title": "Trilinear rendering",
    "section": "Siddon‚Äôs method",
    "text": "Siddon‚Äôs method\n\n# Initialize the DRR module for generating synthetic X-rays\ndrr = DRR(\n    subject,\n    sdd=1020.0,\n    height=200,\n    delx=2.0,\n).to(device)\n\nsource, target = drr.detector(pose)\n_ = drr(pose)  # Initialize drr.density\n\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose)\nplot_drr(img, title=f\"Siddon ({time})\")\nplt.show()\n\n37.6 ms ¬± 18.7 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "tutorials/trilinear.html#trilinear-interpolation",
    "href": "tutorials/trilinear.html#trilinear-interpolation",
    "title": "Trilinear rendering",
    "section": "Trilinear interpolation",
    "text": "Trilinear interpolation\n\ndrr = DRR(\n    subject,\n    sdd=1020.0,\n    height=200,\n    delx=2.0,\n    renderer=\"trilinear\",  # Switch the rendering mode\n).to(device)\n\nsource, target = drr.detector(pose)\n_ = drr(pose)  # Initialize drr.density\n\n\nn_points = 25\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n445 ¬µs ¬± 2.37 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\n\n\n\n\n\n\nn_points = 50\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n743 ¬µs ¬± 572 ns per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\n\n\n\n\n\n\nn_points = 100\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n1.47 ms ¬± 4.15 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\n\n\n\n\n\n\nn_points = 200\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n4.45 ms ¬± 675 ns per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nn_points = 250\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n6.3 ms ¬± 380 ns per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nn_points = 500\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n13.1 ms ¬± 3.71 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nn_points = 1000\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n21 ms ¬± 4.25 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n\n\n\n\n\nn_points = 2000\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n31.4 ms ¬± 24.4 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n\n\n\n\n\nn_points = 5000\n\ntimes = %timeit -o drr.renderer(drr.density, drr.origin, drr.spacing, source, target, n_points)\ntime = f\"{_format_time(times.average, times._precision)} ¬± {_format_time(times.stdev, times._precision)}\"\n\nimg = drr(pose, n_points=n_points)\nplot_drr(img, title=f\"Trilinear with {n_points} points ({time})\")\nplt.show()\n\n70.3 ms ¬± 8.87 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "tutorials/reconstruction.html",
    "href": "tutorials/reconstruction.html",
    "title": "3D reconstruction",
    "section": "",
    "text": "To perform 3D reconstruction with DiffDRR, we do the following:",
    "crumbs": [
      "tutorials",
      "3D reconstruction"
    ]
  },
  {
    "objectID": "tutorials/reconstruction.html#generate-a-target-x-ray",
    "href": "tutorials/reconstruction.html#generate-a-target-x-ray",
    "title": "3D reconstruction",
    "section": "1. Generate a target X-ray",
    "text": "1. Generate a target X-ray\n\n\nCode\nimport matplotlib.pyplot as plt\nimport torch\nimport torchio\nfrom tqdm import tqdm\n\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.drr import DRR\nfrom diffdrr.visualization import plot_drr\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nsubject = load_example_ct()\ndrr = DRR(subject, sdd=1020.0, height=200, delx=2.0).to(device)\n\nrotations = torch.tensor([[0.0, 0.0, 0.0]], device=device)\ntranslations = torch.tensor([[0.0, 850.0, 0.0]], device=device)\ngt = drr(rotations, translations, parameterization=\"euler_angles\", convention=\"ZXY\")\ngt = (gt - gt.min()) / (gt.max() - gt.min())\nplot_drr(gt, ticks=False)\nplt.show()",
    "crumbs": [
      "tutorials",
      "3D reconstruction"
    ]
  },
  {
    "objectID": "tutorials/reconstruction.html#initialize-a-moving-drr-from-a-random-volume",
    "href": "tutorials/reconstruction.html#initialize-a-moving-drr-from-a-random-volume",
    "title": "3D reconstruction",
    "section": "2. Initialize a moving DRR from a random volume",
    "text": "2. Initialize a moving DRR from a random volume\nNote that we have to use renderer=\"trilinear\" for volume reconstruction‚Ä¶ bonus points if you can figure out why!\nBelow is an implementation of a simple volume reconstruction module with DiffDRR.\n\nfrom diffdrr.pose import convert\n\n\nclass Reconstruction(torch.nn.Module):\n    def __init__(self, subject, device):\n        super().__init__()\n\n        # Replace the known density with an initial estimate\n        self.density = torch.nn.Parameter(\n            torch.zeros(*subject.volume.shape, device=device)[0]\n        )\n\n        self.drr = DRR(\n            subject,\n            sdd=1020.0,\n            height=200,\n            delx=2.0,\n            renderer=\"trilinear\",\n        ).to(device)\n\n    def forward(self, pose, **kwargs):\n        source, target = self.drr.detector(pose, None)\n        img = self.drr.renderer(\n            self.density,\n            self.drr.origin,\n            self.drr.spacing,\n            source,\n            target,\n            **kwargs,\n        )\n        return self.drr.reshape_transform(img, batch_size=len(pose))",
    "crumbs": [
      "tutorials",
      "3D reconstruction"
    ]
  },
  {
    "objectID": "tutorials/reconstruction.html#and-4.-optimize",
    "href": "tutorials/reconstruction.html#and-4.-optimize",
    "title": "3D reconstruction",
    "section": "3 and 4. Optimize!",
    "text": "3 and 4. Optimize!\n\nRender DRRs from the given camera poses\nMeasure the loss between projected DRRs and the ground truth X-ray (here we use MSE)\nUpdate the estimate for the volume\nRepeat until converged!\n\n\n\nCode\nrecon = Reconstruction(subject, device)\noptimizer = torch.optim.Adam(recon.parameters(), lr=1e-3)\ncriterion = torch.nn.MSELoss()\n\nrotations = torch.tensor([[0.0, 0.0, 0.0]], device=device)\ntranslations = torch.tensor([[0.0, 850.0, 0.0]], device=device)\npose = convert(\n    rotations, translations, parameterization=\"euler_angles\", convention=\"ZXY\"\n)\n\nlosses = []\nfor itr in tqdm(range(101)):\n    optimizer.zero_grad()\n    est = recon(pose)\n    loss = criterion(est, gt)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    if itr % 25 == 0:\n        plot_drr(torch.concat([est, gt]), title=[\"Reconstruction\", \"Ground Truth\"])\n        plt.show()\n\nplt.plot(losses)\nplt.xlabel(\"# Iterations\")\nplt.ylabel(\"MSE\")\nplt.yscale(\"log\")\nplt.show()\n\n\n  0%|                                                                                         | 0/101 [00:00&lt;?, ?it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                            | 25/101 [00:01&lt;00:03, 20.60it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                         | 49/101 [00:02&lt;00:02, 20.53it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 73/101 [00:04&lt;00:01, 20.53it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 100/101 [00:05&lt;00:00, 20.84it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:05&lt;00:00, 17.22it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter optimizing for 100 iterations, we get a DRR that matches the input X-ray‚Ä¶ even after starting with a randomly initialized voxelgrid! This demonstrates that differentiable rendering for volume reconstruction works with DiffDRR. But have we actually reconstructed something useful?",
    "crumbs": [
      "tutorials",
      "3D reconstruction"
    ]
  },
  {
    "objectID": "tutorials/reconstruction.html#application-novel-view-synthesis",
    "href": "tutorials/reconstruction.html#application-novel-view-synthesis",
    "title": "3D reconstruction",
    "section": "Application: Novel View Synthesis",
    "text": "Application: Novel View Synthesis\nOne way we can test the robustness of our reconstruction is by rendering DRRs from different poses.\nFirst, let‚Äôs try bringing the C-arm 10 mm closer to the patient. Instantly, we can see that the intensities of the rendered images looks off‚Ä¶\n\nrotations = torch.tensor([[0.0, 0.0, 0.0]], device=device)\ntranslations = torch.tensor([[0.0, 840.0, 0.0]], device=device)\npose = convert(\n    rotations, translations, parameterization=\"euler_angles\", convention=\"ZXY\"\n)\n\nplot_drr(\n    torch.concat([recon(pose), drr(pose)]),\n    title=[\"Reconstruction\", \"Ground Truth\"],\n)\nplt.show()\n\n\n\n\n\n\n\n\nNow let‚Äôs try rotating the detector by 1 degree. Issues are even more apparent here!\n\nrotations = torch.tensor([[1.0, 0.0, 0.0]], device=device) / 180 * torch.pi\ntranslations = torch.tensor([[0.0, 850.0, 0.0]], device=device)\npose = convert(\n    rotations, translations, parameterization=\"euler_angles\", convention=\"ZXY\"\n)\n\nplot_drr(\n    torch.concat([recon(pose), drr(pose)]),\n    title=[\"Reconstruction\", \"Ground Truth\"],\n)\nplt.show()\n\n\n\n\n\n\n\n\nThese results should not be surprising. After all, we were trying to reconstruct a 3D volume from a single X-ray. Real reconstruction algorithms typically require &gt;100 images to achieve good novel view synthesis. Methods that achieve reconstruction with &lt;100 images typically have some neural shenanigans going on (which one can totally do with DiffDRR!).",
    "crumbs": [
      "tutorials",
      "3D reconstruction"
    ]
  },
  {
    "objectID": "tutorials/reconstruction.html#so-what-did-we-reconstruct",
    "href": "tutorials/reconstruction.html#so-what-did-we-reconstruct",
    "title": "3D reconstruction",
    "section": "So what did we reconstruct?",
    "text": "So what did we reconstruct?\nWe visualize slices from the volume reconstructed with DiffDRR and the original CT. The results are amusing! The takeaway is that the volume we reconstructed is incredibly overfit to produce the target X-ray. Every other X-ray that one might want to render is going to suffer horrible artifacts because our reconstruction isn‚Äôt generalized at all.\n\n\nCode\nfor jdx in [0, 20, 40, 60, 80, 100, 120]:\n    plt.subplot(121)\n    plt.imshow(recon.density[..., jdx].detach().cpu(), cmap=\"gray\")\n    plt.ylabel(f\"Slice = {jdx}\")\n    plt.title(\"Reconstruction\")\n    plt.xticks([])\n    plt.yticks([])\n    plt.subplot(122)\n    plt.imshow(drr.density[..., jdx].detach().cpu(), cmap=\"gray\")\n    plt.title(\"Real CT\")\n    plt.axis(\"off\")\n    plt.show()",
    "crumbs": [
      "tutorials",
      "3D reconstruction"
    ]
  },
  {
    "objectID": "tutorials/metrics.html",
    "href": "tutorials/metrics.html",
    "title": "Registration loss landscapes",
    "section": "",
    "text": "We visualize the loss landscape by simulating a synthetic registration problem. To do this, we start by simulating a DRR from a known angle, which will serve as the target for registration.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport torch\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom tqdm import tqdm\n\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.drr import DRR\nfrom diffdrr.metrics import MultiscaleNormalizedCrossCorrelation2d\nfrom diffdrr.visualization import plot_drr\n\n# Read in the volume and get its origin and spacing in world coordinates\nsubject = load_example_ct()\n\n# Initialize the DRR module for generating synthetic X-rays\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr = DRR(\n    subject,  # A diffdrr.data.Subject object storing the CT volume, origin, and voxel spacing\n    sdd=1020,  # Source-to-detector distance (i.e., the C-arm's focal length)\n    height=200,  # Height of the DRR (if width is not seperately provided, the generated image is square)\n    delx=2.0,  # Pixel spacing (in mm)\n).to(device)\n\n# Get parameters for the detector\nalpha, beta, gamma = 0.0, 0.0, 0.0\nbx, by, bz = 0.0, 850.0, 0.0\nrotations = torch.tensor([[alpha, beta, gamma]], device=device)\ntranslations = torch.tensor([[bx, by, bz]], device=device)\n\n# MNake the DRR\ntarget_drr = drr(\n    rotations, translations, parameterization=\"euler_angles\", convention=\"ZYX\"\n)\nplot_drr(target_drr, ticks=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nNext, we create a function that measures the normalized cross-correlation between the target DRR and a moving DRR, simulated by some perturbation from the true parameters of the target DRR.\n\nmetric = MultiscaleNormalizedCrossCorrelation2d([13, None], [0.5, 0.5])\n\n\ndef get_metric(alpha, beta, gamma, bx, by, bz):\n    rotations = torch.tensor([[alpha, beta, gamma]]).to(device)\n    translations = torch.tensor([[bx, by, bz]]).to(device)\n    moving_drr = drr(\n        rotations, translations, parameterization=\"euler_angles\", convention=\"ZXY\"\n    )\n    return metric(target_drr, moving_drr)\n\nFinally, we can simulate hundreds of moving DRRs and measure their cross correlation with the target. Plotting the cross correlation versus the perturbation from the true DRR parameters allows us to visualize the loss landscape for the six pose parameters. From this visualization, we see that the loss landscape is convex in this neighborhood (¬±15 mm and ¬±180 degrees).\n\n\nCode\n### NCC for the XYZs\nxs = torch.arange(-15.0, 16.0, step=2)\nys = torch.arange(-15.0, 16.0, step=2)\nzs = torch.arange(-15.0, 16.0, step=2)\n\n# Get coordinate-wise correlations\nxy_corrs = []\nfor x in tqdm(xs, desc=\"XY\", ncols=50):\n    for y in ys:\n        xcorr = get_metric(alpha, beta, gamma, bx + x, by + y, bz)\n        xy_corrs.append(-xcorr)\nXY = torch.tensor(xy_corrs).reshape(len(xs), len(ys))\n\nxz_corrs = []\nfor x in tqdm(xs, desc=\"XZ\", ncols=50):\n    for z in zs:\n        xcorr = get_metric(alpha, beta, gamma, bx + x, by, bz + z)\n        xz_corrs.append(-xcorr)\nXZ = torch.tensor(xz_corrs).reshape(len(xs), len(zs))\n\nyz_corrs = []\nfor y in tqdm(ys, desc=\"YZ\", ncols=50):\n    for z in zs:\n        xcorr = get_metric(alpha, beta, gamma, bx, by + y, bz + z)\n        yz_corrs.append(-xcorr)\nYZ = torch.tensor(yz_corrs).reshape(len(ys), len(zs))\n\n### NCC for the angles\na_angles = torch.arange(-torch.pi / 4, torch.pi / 4, step=0.05)\nb_angles = torch.arange(-torch.pi / 4, torch.pi / 4, step=0.05)\ng_angles = torch.arange(-torch.pi / 8, torch.pi / 8, step=0.05)\n\n# Get coordinate-wise correlations\ntp_corrs = []\nfor t in tqdm(a_angles, desc=\"Œ±Œ≤\", ncols=50):\n    for p in b_angles:\n        xcorr = get_metric(alpha + t, beta + p, gamma, bx, by, bz)\n        tp_corrs.append(-xcorr)\nTP = torch.tensor(tp_corrs).reshape(len(a_angles), len(b_angles))\n\ntg_corrs = []\nfor t in tqdm(a_angles, desc=\"Œ±Œ≥\", ncols=50):\n    for g in g_angles:\n        xcorr = get_metric(alpha + t, beta, gamma + g, bx, by, bz)\n        tg_corrs.append(-xcorr)\nTG = torch.tensor(tg_corrs).reshape(len(a_angles), len(g_angles))\n\npg_corrs = []\nfor p in tqdm(b_angles, desc=\"Œ≤Œ≥\", ncols=50):\n    for g in g_angles:\n        xcorr = get_metric(alpha, beta + p, gamma + g, bx, by, bz)\n        pg_corrs.append(-xcorr)\nPG = torch.tensor(pg_corrs).reshape(len(b_angles), len(g_angles))\n\n### Make the plots\n\n# XYZ\nxyx, xyy = torch.meshgrid(xs, ys, indexing=\"ij\")\nxzx, xzz = torch.meshgrid(xs, zs, indexing=\"ij\")\nyzy, yzz = torch.meshgrid(ys, zs, indexing=\"ij\")\n\nfig = plt.figure(figsize=3 * plt.figaspect(1.2 / 1), dpi=300)\n\nax = fig.add_subplot(1, 3, 1, projection=\"3d\")\nax.contourf(xyx, xyy, XY, zdir=\"z\", offset=-1, cmap=plt.get_cmap(\"rainbow\"), alpha=0.5)\nax.plot_surface(xyx, xyy, XY, rstride=1, cstride=1, cmap=plt.get_cmap(\"rainbow\"))\nax.set_xlabel(\"ŒîX (mm)\")\nax.set_ylabel(\"ŒîY (mm)\")\nax.set_zlim3d(-1.0, -0.7)\n\nax = fig.add_subplot(1, 3, 2, projection=\"3d\")\nax.contourf(xzx, xzz, XZ, zdir=\"z\", offset=-1, cmap=plt.get_cmap(\"rainbow\"), alpha=0.5)\nax.plot_surface(xzx, xzz, XZ, rstride=1, cstride=1, cmap=plt.get_cmap(\"rainbow\"))\nax.set_xlabel(\"ŒîX (mm)\")\nax.set_ylabel(\"ŒîZ (mm)\")\nax.set_zlim3d(-1.0, -0.7)\n\nax = fig.add_subplot(1, 3, 3, projection=\"3d\")\nax.contourf(yzy, yzz, YZ, zdir=\"z\", offset=-1, cmap=plt.get_cmap(\"rainbow\"), alpha=0.5)\nax.plot_surface(yzy, yzz, YZ, rstride=1, cstride=1, cmap=plt.get_cmap(\"rainbow\"))\nax.set_xlabel(\"ŒîY (mm)\")\nax.set_ylabel(\"ŒîZ (mm)\")\nax.set_zlim3d(-1.0, -0.7)\n\n# Angles\nxyx, xyy = torch.meshgrid(a_angles, b_angles, indexing=\"ij\")\nxzx, xzz = torch.meshgrid(a_angles, g_angles, indexing=\"ij\")\nyzy, yzz = torch.meshgrid(b_angles, g_angles, indexing=\"ij\")\n\nax = fig.add_subplot(2, 3, 1, projection=\"3d\")\nax.contourf(xyx, xyy, TP, zdir=\"z\", offset=-1, cmap=plt.get_cmap(\"rainbow\"), alpha=0.5)\nax.plot_surface(xyx, xyy, TP, rstride=1, cstride=1, cmap=plt.get_cmap(\"rainbow\"))\nax.set_xlabel(\"ŒîŒ± (radians)\")\nax.set_ylabel(\"ŒîŒ≤ (radians)\")\nax.set_zlim3d(-1.0, 0.25)\n\nax = fig.add_subplot(2, 3, 2, projection=\"3d\")\nax.contourf(xzx, xzz, TG, zdir=\"z\", offset=-1, cmap=plt.get_cmap(\"rainbow\"), alpha=0.5)\nax.plot_surface(xzx, xzz, TG, rstride=1, cstride=1, cmap=plt.get_cmap(\"rainbow\"))\nax.set_xlabel(\"ŒîŒ± (radians)\")\nax.set_ylabel(\"ŒîŒ≥ (radians)\")\nax.set_zlim3d(-1.0, 0.25)\n\nax = fig.add_subplot(2, 3, 3, projection=\"3d\")\nax.contourf(yzy, yzz, PG, zdir=\"z\", offset=-1, cmap=plt.get_cmap(\"rainbow\"), alpha=0.5)\nax.plot_surface(yzy, yzz, PG, rstride=1, cstride=1, cmap=plt.get_cmap(\"rainbow\"))\nax.set_xlabel(\"ŒîŒ≤ (radians)\")\nax.set_ylabel(\"ŒîŒ≥ (radians)\")\nax.set_zlim3d(-1.0, 0.25)\n\nplt.show()\n\n\nXY: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09&lt;00:00,  1.60it/s]\nXZ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10&lt;00:00,  1.60it/s]\nYZ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09&lt;00:00,  1.60it/s]\nŒ±Œ≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:20&lt;00:00,  1.53it/s]\nŒ±Œ≥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:14&lt;00:00,  2.25it/s]\nŒ≤Œ≥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:14&lt;00:00,  2.26it/s]",
    "crumbs": [
      "tutorials",
      "Registration loss landscapes"
    ]
  },
  {
    "objectID": "tutorials/metamorphasis.html",
    "href": "tutorials/metamorphasis.html",
    "title": "Converting to DeepDRR",
    "section": "",
    "text": "from inspect import getfile\nfrom pathlib import Path\n\nimport deepdrr\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom deepdrr import MobileCArm, Projector, Volume, geo\nfrom deepdrr.load_dicom import load_dicom\n\nfrom diffdrr.detector import diffdrr_to_deepdrr\nfrom diffdrr.drr import DRR\nfrom diffdrr.metrics import NormalizedCrossCorrelation2d\n\n\nSDR = 400\nP = 4.0\n\n\n# Load a DICOM and extract voxel information\nexample_ct_path = str(Path(getfile(DRR)).parent / \"data/cxr\") + \"/\"\nvolume, materials, spacing = load_dicom(example_ct_path)\n\n# Make volume conventions same as DiffDR\npreprocess = lambda x: np.rot90(x, -1)[:, ::-1]\nvolume = preprocess(volume)\nfor key, value in materials.items():\n    materials[key] = preprocess(value)\n\n# Use the center of the volume as the \"world\" coordinates. The origin is the (0, 0, 0) index of the volume in the world frame.\nvol_center = (np.array(volume.shape) - 1) / 2 * spacing\norigin = geo.point(-vol_center[0], -vol_center[1], -vol_center[2])\n\n# Create the volume object with segmentation\npatient = Volume.from_parameters(\n    data=volume,\n    materials=materials,\n    origin=origin,\n    spacing=spacing,\n    anatomical_coordinate_system=\"LPS\",\n)\npatient.orient_patient(head_first=True, supine=True)\n\nUsing downloaded and verified file: /home/vivekg/datasets/DeepDRR_DATA/model_segmentation.pth.tar\n\n\n\n# defines the C-Arm device, which is a convenience class for positioning the Camera.\n# isocenter=volume.center_in_world\ncarm = MobileCArm(\n    isocenter=patient.center_in_world,\n    rotate_camera_left=False,\n    source_to_detector_distance=SDR * 2,\n    source_to_isocenter_vertical_distance=SDR,\n    pixel_size=P,\n    sensor_height=256,\n    sensor_width=256,\n    min_alpha=-720,\n    max_alpha=720,\n    min_beta=-720,\n    max_beta=720,\n)\n\n\ndef test_phantom_deepdrr(alpha, beta, gamma):\n    with Projector(\n        volume=patient,\n        carm=carm,\n    ) as projector:\n        carm.move_to(\n            alpha=np.rad2deg(alpha),\n            beta=np.rad2deg(np.pi / 2 - beta),\n            gamma=np.rad2deg(-gamma),\n            degrees=True,\n        )\n        img = (\n            projector()\n        )  # The first run doesn't use updated parameters, for some reason?\n        img = projector()[:, ::-1].copy()\n    return img\n\n\ndef test_phantom_diffdrr(alpha, beta, gamma, sdr=SDR, p=P):\n    bx, by, bz = (torch.tensor(volume.shape) - 1) * torch.tensor(spacing) / 2\n    drr = DRR(volume, spacing, sdr=SDR, height=256, delx=P, convention=\"deepdrr\")\n    img = drr(\n        diffdrr_to_deepdrr(torch.tensor([[alpha, beta, gamma]])),\n        torch.tensor([[bx, by, bz]]),\n        parameterization=\"euler_angles\",\n        convention=\"YZX\",\n    )\n    img = img / img.max()\n    return img\n\n\nfor idx in range(5):\n    alpha = np.random.uniform(-torch.pi, torch.pi)\n    beta = np.random.uniform(-torch.pi, torch.pi)\n    gamma = np.random.uniform(-torch.pi, torch.pi)\n    diff = test_phantom_diffdrr(alpha, beta, gamma).squeeze().numpy()\n    deep = test_phantom_deepdrr(alpha, beta, gamma)\n    metric = NormalizedCrossCorrelation2d()(\n        torch.tensor(diff[np.newaxis, np.newaxis, ...]),\n        torch.tensor(deep[np.newaxis, np.newaxis, ...]),\n    ).item()\n\n    plt.figure(figsize=(12, 3))\n    plt.subplot(131)\n    plt.title(\"DiffDRR\")\n    plt.imshow(diff, cmap=\"gray\")\n    plt.colorbar()\n    plt.subplot(132)\n    plt.title(\"DeepDRR\")\n    plt.imshow(deep, cmap=\"gray\")\n    plt.colorbar()\n    plt.subplot(133)\n    plt.title(f\"NCC = {metric:.5g}\")\n    plt.imshow(deep - diff, cmap=\"gray\")\n    plt.colorbar()\n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "tutorials",
      "Converting to `DeepDRR`"
    ]
  },
  {
    "objectID": "api/metrics.html",
    "href": "api/metrics.html",
    "title": "metrics",
    "section": "",
    "text": "Compute the similarity between a fixed X-ray \\(\\mathbf I\\) and a moving X-ray \\(\\mathbf{\\hat I}\\), where \\(\\mathbf{\\hat I}\\) is rendered from an estimated camera pose (registration) or volume (reconstruction).\nWe implement patchwise variants of the following metrics:\n\nNormalized Cross Correlation (NCC)\nMultiscale Normalized Cross Correlation (mNCC)\nGradient Normalized Cross Correlation (gNCC)\n\n\n\n\n\n\n\nTip\n\n\n\nIf patch_size=None, the similarity metric is computed over the entire image.\n\n\n\nsource\n\n\n\n NormalizedCrossCorrelation2d (patch_size=None, eps=1e-05)\n\nCompute Normalized Cross Correlation between two batches of images.\n\nsource\n\n\n\n\n MultiscaleNormalizedCrossCorrelation2d (patch_sizes=[None],\n                                         patch_weights=[1.0], eps=1e-05)\n\nCompute Normalized Cross Correlation between two batches of images at multiple scales.\n\nsource\n\n\n\n\n GradientNormalizedCrossCorrelation2d (patch_size=None, sigma=1.0,\n                                       **kwargs)\n\nCompute Normalized Cross Correlation between the image gradients of two batches of images.",
    "crumbs": [
      "api",
      "metrics"
    ]
  },
  {
    "objectID": "api/metrics.html#image-similarity-metrics",
    "href": "api/metrics.html#image-similarity-metrics",
    "title": "metrics",
    "section": "",
    "text": "Compute the similarity between a fixed X-ray \\(\\mathbf I\\) and a moving X-ray \\(\\mathbf{\\hat I}\\), where \\(\\mathbf{\\hat I}\\) is rendered from an estimated camera pose (registration) or volume (reconstruction).\nWe implement patchwise variants of the following metrics:\n\nNormalized Cross Correlation (NCC)\nMultiscale Normalized Cross Correlation (mNCC)\nGradient Normalized Cross Correlation (gNCC)\n\n\n\n\n\n\n\nTip\n\n\n\nIf patch_size=None, the similarity metric is computed over the entire image.\n\n\n\nsource\n\n\n\n NormalizedCrossCorrelation2d (patch_size=None, eps=1e-05)\n\nCompute Normalized Cross Correlation between two batches of images.\n\nsource\n\n\n\n\n MultiscaleNormalizedCrossCorrelation2d (patch_sizes=[None],\n                                         patch_weights=[1.0], eps=1e-05)\n\nCompute Normalized Cross Correlation between two batches of images at multiple scales.\n\nsource\n\n\n\n\n GradientNormalizedCrossCorrelation2d (patch_size=None, sigma=1.0,\n                                       **kwargs)\n\nCompute Normalized Cross Correlation between the image gradients of two batches of images.",
    "crumbs": [
      "api",
      "metrics"
    ]
  },
  {
    "objectID": "api/metrics.html#geodesic-distances-for-se3",
    "href": "api/metrics.html#geodesic-distances-for-se3",
    "title": "metrics",
    "section": "Geodesic distances for SE(3)",
    "text": "Geodesic distances for SE(3)\nOne can define geodesic pseudo-distances on \\(\\mathbf{SO}(3)\\) and \\(\\mathbf{SE}(3)\\).1 This let‚Äôs us measure registration error (in radians and millimeters, respectively) on poses, rather than needed to compute the projection of fiducials.\nWe implement two geodesics on \\(\\mathbf{SE}(3)\\):\n\nThe logarithmic geodesic\nThe double geodesic\n\n\nLogarithmic Geodesic\nGiven two rotation matrices \\(\\mathbf R_A, \\mathbf R_B \\in \\mathbf{SO}(3)\\), the angular distance between their axes of rotation is\n\\[\n    d_\\theta(\\mathbf R_A, \\mathbf R_B)\n    = \\arccos \\left( \\frac{\\mathrm{trace}(\\mathbf R_A^T \\mathbf R_B) - 1}{2} \\right)\n    = \\| \\log (\\mathbf R_A^T \\mathbf R_B) \\| \\,,\n\\]\nwhere \\(\\log(\\cdot)\\) is the logarithm map on \\(\\mathbf{SO}(3)\\).2 Using the logarithm map on \\(\\mathbf{SE}(3)\\), this generalizes to a geodesic loss function on camera poses \\({\\mathbf T}_A, {\\mathbf T}_B \\in \\mathbf{SE}(3)\\):\n\\[\n    \\mathcal L_{\\mathrm{log}}({\\mathbf T}_A, {\\mathbf T}_B) = \\| \\log({\\mathbf T}_A^{-1} {\\mathbf T}_B) \\| \\,.\n\\]\n\nsource\n\n\nLogGeodesicSE3\n\n LogGeodesicSE3 ()\n\nCalculate the distance between transforms in the log-space of SE(3).\n\n# SE(3) distance\ngeodesic_se3 = LogGeodesicSE3()\n\npose_1 = convert(\n    torch.tensor([[0.1, 1.0, torch.pi]]),\n    torch.ones(1, 3),\n    parameterization=\"euler_angles\",\n    convention=\"ZYX\",\n)\npose_2 = convert(\n    torch.tensor([[0.1, 1.1, torch.pi]]),\n    torch.zeros(1, 3),\n    parameterization=\"euler_angles\",\n    convention=\"ZYX\",\n)\n\ngeodesic_se3(pose_1, pose_2)\n\ntensor([1.7355])",
    "crumbs": [
      "api",
      "metrics"
    ]
  },
  {
    "objectID": "api/metrics.html#double-geodesic",
    "href": "api/metrics.html#double-geodesic",
    "title": "metrics",
    "section": "Double Geodesic",
    "text": "Double Geodesic\nWe can also formulate a geodesic distance on \\(\\mathbf{SE}(3)\\) with units of length. Using the camera‚Äôs focal length \\(f\\), we convert the angular distance to an arc length:\n\\[\n    d_\\theta(\\mathbf R_A, \\mathbf R_B; f) = \\frac{f}{2} d_\\theta(\\mathbf R_A, \\mathbf R_B) \\,.\n\\]\nWhen combined with the Euclidean distance on the translations \\(d_t(\\mathbf t_A, \\mathbf t_B) = \\| \\mathbf t_A - \\mathbf t_B \\|\\), this yields the double geodesic loss on \\(\\mathbf{SE}(3)\\):3\n\\[\n    \\mathcal L_{\\mathrm{geo}}({\\mathbf T}_A, {\\mathbf T}_B; f) = \\sqrt{d^2_\\theta(\\mathbf R_A, \\mathbf R_B; f) + d^2_t(\\mathbf t_A, \\mathbf t_B)} \\,.\n\\]\n\nsource\n\nDoubleGeodesicSE3\n\n DoubleGeodesicSE3 (sdd:float, eps:float=0.0001)\n\nCalculate the angular and translational geodesics between two SE(3) transformation matrices.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsdd\nfloat\n\nSource-to-detector distance\n\n\neps\nfloat\n0.0001\nAvoid overflows in sqrt\n\n\n\n\n# Angular distance and translational distance both in mm\ndouble_geodesic = DoubleGeodesicSE3(1020 / 2)\n\npose_1 = convert(\n    torch.tensor([[0.1, 1.0, torch.pi]]),\n    torch.ones(1, 3),\n    parameterization=\"euler_angles\",\n    convention=\"ZYX\",\n)\npose_2 = convert(\n    torch.tensor([[0.1, 1.1, torch.pi]]),\n    torch.zeros(1, 3),\n    parameterization=\"euler_angles\",\n    convention=\"ZYX\",\n)\n\ndouble_geodesic(pose_1, pose_2)  # Angular, translational, double geodesics\n\n(tensor([25.5000]), tensor([1.7321]), tensor([25.5588]))",
    "crumbs": [
      "api",
      "metrics"
    ]
  },
  {
    "objectID": "api/metrics.html#footnotes",
    "href": "api/metrics.html#footnotes",
    "title": "metrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://vnav.mit.edu/material/04-05-LieGroups-notes.pdf‚Ü©Ô∏é\nhttps://www.cs.cmu.edu/~cga/dynopt/readings/Rmetric.pdf‚Ü©Ô∏é\nhttps://rpk.lcsr.jhu.edu/wp-content/uploads/2017/08/Partial-Bi-Invariance-of-SE3-Metrics1.pdf‚Ü©Ô∏é",
    "crumbs": [
      "api",
      "metrics"
    ]
  },
  {
    "objectID": "api/renderers.html",
    "href": "api/renderers.html",
    "title": "renderers",
    "section": "",
    "text": "DRRs are generated by modeling the geometry of an idealized projectional radiography system. Let \\(\\mathbf s \\in \\mathbb R^3\\) be the X-ray source and \\(\\mathbf p \\in \\mathbb R^3\\) be a target pixel on the detector plane. Then, \\(R(\\alpha) = \\mathbf s + \\alpha (\\mathbf p - \\mathbf s)\\) is a ray that originates from \\(\\mathbf s\\) (\\(\\alpha=0\\)), passes through the imaged volume, and hits the detector plane at \\(\\mathbf p\\) (\\(\\alpha=1\\)). The proportion of energy attenuation experienced by the X-ray at the time it reaches pixel \\(\\mathbf p\\) is given by the following line integral:\n\\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2 \\int_0^1 \\mathbf V \\left( \\mathbf s + \\alpha (\\mathbf p - \\mathbf s) \\right) \\, \\mathrm d\\alpha \\,,\n\\end{equation}\\]\nwhere \\(\\mathbf V : \\mathbb R^3 \\mapsto \\mathbb R\\) is the imaged volume. The units term \\(\\|\\mathbf p - \\mathbf s\\|_2\\) serves to cancel out the units of \\(\\mathbf V(\\cdot)\\), reciprocal length, such that the final proportion \\(E\\) is unitless. For DRR synthesis, \\(\\mathbf V\\) is approximated by a discrete 3D CT volume, and the first equation becomes\n\\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2 \\sum_{m=1}^{M-1} (\\alpha_{m+1} - \\alpha_m) \\mathbf V \\left[ \\mathbf s + \\frac{\\alpha_{m+1} + \\alpha_m}{2} (\\mathbf p - \\mathbf s) \\right] \\,,\n\\end{equation}\\]\nwhere \\(\\alpha_m\\) parameterizes the locations where ray \\(R\\) intersects one of the orthogonal planes comprising the CT volume, and \\(M\\) is the number of such intersections.\n\n\n\n\n\n\nNote\n\n\n\nNote that this model does not account for patterns of reflection and scattering that are present in real X-ray systems. While these simplifications preclude synthesis of realistic X-rays, the model in Siddon‚Äôs method has been widely and successfully used in slice-to-volume registration.\n\n\nSiddon‚Äôs method provides a parametric method to identify the plane intersections \\(\\{\\alpha_m\\}_{m=1}^M\\). Let \\(\\Delta X\\) be the CT voxel size in the \\(x\\)-direction and \\(b_x\\) be the location of the \\(0\\)-th plane in this direction. Then the intersection of ray \\(R\\) with the \\(i\\)-th plane in the \\(x\\)-direction is given by \\[\\begin{equation}\n    \\alpha_x(i) = \\frac{b_x + i \\Delta X - \\mathbf s_x}{\\mathbf p_x - \\mathbf s_x} ,\n\\end{equation}\\] with analogous expressions for \\(\\alpha_y(\\cdot)\\) and \\(\\alpha_z(\\cdot)\\).\nWe can use this equation to compute the values \\(\\mathbf \\alpha_x\\) for all the intersections between \\(R\\) and the planes in the \\(x\\)-direction: \\[\\begin{equation}\n    \\mathbf\\alpha_x = \\{ \\alpha_x(i_{\\min}), \\dots, \\alpha_x(i_{\\max}) \\} ,\n\\end{equation}\\] where \\(i_{\\min}\\) and \\(i_{\\max}\\) denote the first and last intersections of \\(R\\) with the \\(x\\)-direction planes.\nDefining \\(\\mathbf\\alpha_y\\) and \\(\\mathbf\\alpha_z\\) analogously, we construct the array \\[\\begin{equation}\n    \\mathbf\\alpha = \\mathrm{sort}(\\mathbf\\alpha_x, \\mathbf\\alpha_y, \\mathbf\\alpha_z) ,\n\\end{equation}\\] which contains \\(M\\) values of \\(\\alpha\\) parameterizing the intersections between \\(R\\) and the orthogonal \\(x\\)-, \\(y\\)-, and \\(z\\)-directional planes. We substitute values in the sorted set \\(\\mathbf\\alpha\\) into the first equation to evaluate \\(E(R)\\), which corresponds to the intensity of pixel \\(\\mathbf p\\) in the synthesized DRR.\n\nsource\n\n\n\n Siddon (eps=1e-08)\n\nDifferentiable X-ray renderer implemented with Siddon‚Äôs method for exact raytracing.",
    "crumbs": [
      "api",
      "renderers"
    ]
  },
  {
    "objectID": "api/renderers.html#siddons-method",
    "href": "api/renderers.html#siddons-method",
    "title": "renderers",
    "section": "",
    "text": "DRRs are generated by modeling the geometry of an idealized projectional radiography system. Let \\(\\mathbf s \\in \\mathbb R^3\\) be the X-ray source and \\(\\mathbf p \\in \\mathbb R^3\\) be a target pixel on the detector plane. Then, \\(R(\\alpha) = \\mathbf s + \\alpha (\\mathbf p - \\mathbf s)\\) is a ray that originates from \\(\\mathbf s\\) (\\(\\alpha=0\\)), passes through the imaged volume, and hits the detector plane at \\(\\mathbf p\\) (\\(\\alpha=1\\)). The proportion of energy attenuation experienced by the X-ray at the time it reaches pixel \\(\\mathbf p\\) is given by the following line integral:\n\\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2 \\int_0^1 \\mathbf V \\left( \\mathbf s + \\alpha (\\mathbf p - \\mathbf s) \\right) \\, \\mathrm d\\alpha \\,,\n\\end{equation}\\]\nwhere \\(\\mathbf V : \\mathbb R^3 \\mapsto \\mathbb R\\) is the imaged volume. The units term \\(\\|\\mathbf p - \\mathbf s\\|_2\\) serves to cancel out the units of \\(\\mathbf V(\\cdot)\\), reciprocal length, such that the final proportion \\(E\\) is unitless. For DRR synthesis, \\(\\mathbf V\\) is approximated by a discrete 3D CT volume, and the first equation becomes\n\\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2 \\sum_{m=1}^{M-1} (\\alpha_{m+1} - \\alpha_m) \\mathbf V \\left[ \\mathbf s + \\frac{\\alpha_{m+1} + \\alpha_m}{2} (\\mathbf p - \\mathbf s) \\right] \\,,\n\\end{equation}\\]\nwhere \\(\\alpha_m\\) parameterizes the locations where ray \\(R\\) intersects one of the orthogonal planes comprising the CT volume, and \\(M\\) is the number of such intersections.\n\n\n\n\n\n\nNote\n\n\n\nNote that this model does not account for patterns of reflection and scattering that are present in real X-ray systems. While these simplifications preclude synthesis of realistic X-rays, the model in Siddon‚Äôs method has been widely and successfully used in slice-to-volume registration.\n\n\nSiddon‚Äôs method provides a parametric method to identify the plane intersections \\(\\{\\alpha_m\\}_{m=1}^M\\). Let \\(\\Delta X\\) be the CT voxel size in the \\(x\\)-direction and \\(b_x\\) be the location of the \\(0\\)-th plane in this direction. Then the intersection of ray \\(R\\) with the \\(i\\)-th plane in the \\(x\\)-direction is given by \\[\\begin{equation}\n    \\alpha_x(i) = \\frac{b_x + i \\Delta X - \\mathbf s_x}{\\mathbf p_x - \\mathbf s_x} ,\n\\end{equation}\\] with analogous expressions for \\(\\alpha_y(\\cdot)\\) and \\(\\alpha_z(\\cdot)\\).\nWe can use this equation to compute the values \\(\\mathbf \\alpha_x\\) for all the intersections between \\(R\\) and the planes in the \\(x\\)-direction: \\[\\begin{equation}\n    \\mathbf\\alpha_x = \\{ \\alpha_x(i_{\\min}), \\dots, \\alpha_x(i_{\\max}) \\} ,\n\\end{equation}\\] where \\(i_{\\min}\\) and \\(i_{\\max}\\) denote the first and last intersections of \\(R\\) with the \\(x\\)-direction planes.\nDefining \\(\\mathbf\\alpha_y\\) and \\(\\mathbf\\alpha_z\\) analogously, we construct the array \\[\\begin{equation}\n    \\mathbf\\alpha = \\mathrm{sort}(\\mathbf\\alpha_x, \\mathbf\\alpha_y, \\mathbf\\alpha_z) ,\n\\end{equation}\\] which contains \\(M\\) values of \\(\\alpha\\) parameterizing the intersections between \\(R\\) and the orthogonal \\(x\\)-, \\(y\\)-, and \\(z\\)-directional planes. We substitute values in the sorted set \\(\\mathbf\\alpha\\) into the first equation to evaluate \\(E(R)\\), which corresponds to the intensity of pixel \\(\\mathbf p\\) in the synthesized DRR.\n\nsource\n\n\n\n Siddon (eps=1e-08)\n\nDifferentiable X-ray renderer implemented with Siddon‚Äôs method for exact raytracing.",
    "crumbs": [
      "api",
      "renderers"
    ]
  },
  {
    "objectID": "api/renderers.html#trilinear-interpolation",
    "href": "api/renderers.html#trilinear-interpolation",
    "title": "renderers",
    "section": "Trilinear interpolation",
    "text": "Trilinear interpolation\nInstead of computing the exact line integral over the voxel grid (i.e., Siddon‚Äôs method), we can sample colors at points along the each ray using trilinear interpolation.\nNow, the rendering equation is \\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2\\frac{1}{M} \\sum_{m=1}^{M} \\mathbf V \\left[ \\mathbf s + \\alpha_m (\\mathbf p - \\mathbf s) \\right] \\,,\n\\end{equation}\\] where \\(\\mathbf V[\\cdot]\\) is the trilinear interpolation function and \\(M\\) is the number of points sampled per ray.\n\nsource\n\nTrilinear\n\n Trilinear (near=0.0, far=1.0, mode='bilinear', eps=1e-08)\n\nDifferentiable X-ray renderer implemented with trilinear interpolation.",
    "crumbs": [
      "api",
      "renderers"
    ]
  },
  {
    "objectID": "api/visualization.html",
    "href": "api/visualization.html",
    "title": "visualization",
    "section": "",
    "text": "Uses matplotlib and imageio to plot DRRs and animate optimization over DRRs.\n\nsource\n\n\n\n plot_drr (img:torch.Tensor, title:str|None=None, ticks:bool|None=True,\n           axs:matplotlib.axes._axes.Axes|None=None, cmap:str='gray',\n           **imshow_kwargs)\n\nPlot an image generated by a DRR module.\n\nsource\n\n\n\n\n plot_mask (img:torch.Tensor, axs:matplotlib.axes._axes.Axes,\n            colors=['rgb(102,194,165)', 'rgb(252,141,98)',\n            'rgb(141,160,203)', 'rgb(231,138,195)', 'rgb(166,216,84)',\n            'rgb(255,217,47)', 'rgb(229,196,148)'], alpha=0.625,\n            return_masks=False)\n\nPlot a 2D rendered segmentation mask. Meant to be called after plot_drr.\n\nsource\n\n\n\n\n animate (out:str|pathlib.Path, df:pandas.core.frame.DataFrame,\n          drr:diffdrr.drr.DRR, parameterization:str, convention:str=None,\n          ground_truth:torch.Tensor|None=None, verbose:bool=True,\n          dtype=torch.float32, device='cpu', **kwargs)\n\nAnimate the optimization of a DRR.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nout\nstr | pathlib.Path\n\nSavepath\n\n\ndf\npandas.DataFrame\n\n\n\n\ndrr\nDRR\n\n\n\n\nparameterization\nstr\n\n\n\n\nconvention\nstr\nNone\n\n\n\nground_truth\ntorch.Tensor | None\nNone\n\n\n\nverbose\nbool\nTrue\n\n\n\ndtype\ndtype\ntorch.float32\n\n\n\ndevice\nstr\ncpu\n\n\n\nkwargs\n\n\n\n\n\n\ndf is a pandas.DataFrame with columns [\"alpha\", \"beta\", \"gamma\", \"bx\", \"by\", \"bz\"]. Each row in df is an iteration of optimization with the updated values for that timestep.",
    "crumbs": [
      "api",
      "visualization"
    ]
  },
  {
    "objectID": "api/visualization.html#d-visualization",
    "href": "api/visualization.html#d-visualization",
    "title": "visualization",
    "section": "",
    "text": "Uses matplotlib and imageio to plot DRRs and animate optimization over DRRs.\n\nsource\n\n\n\n plot_drr (img:torch.Tensor, title:str|None=None, ticks:bool|None=True,\n           axs:matplotlib.axes._axes.Axes|None=None, cmap:str='gray',\n           **imshow_kwargs)\n\nPlot an image generated by a DRR module.\n\nsource\n\n\n\n\n plot_mask (img:torch.Tensor, axs:matplotlib.axes._axes.Axes,\n            colors=['rgb(102,194,165)', 'rgb(252,141,98)',\n            'rgb(141,160,203)', 'rgb(231,138,195)', 'rgb(166,216,84)',\n            'rgb(255,217,47)', 'rgb(229,196,148)'], alpha=0.625,\n            return_masks=False)\n\nPlot a 2D rendered segmentation mask. Meant to be called after plot_drr.\n\nsource\n\n\n\n\n animate (out:str|pathlib.Path, df:pandas.core.frame.DataFrame,\n          drr:diffdrr.drr.DRR, parameterization:str, convention:str=None,\n          ground_truth:torch.Tensor|None=None, verbose:bool=True,\n          dtype=torch.float32, device='cpu', **kwargs)\n\nAnimate the optimization of a DRR.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nout\nstr | pathlib.Path\n\nSavepath\n\n\ndf\npandas.DataFrame\n\n\n\n\ndrr\nDRR\n\n\n\n\nparameterization\nstr\n\n\n\n\nconvention\nstr\nNone\n\n\n\nground_truth\ntorch.Tensor | None\nNone\n\n\n\nverbose\nbool\nTrue\n\n\n\ndtype\ndtype\ntorch.float32\n\n\n\ndevice\nstr\ncpu\n\n\n\nkwargs\n\n\n\n\n\n\ndf is a pandas.DataFrame with columns [\"alpha\", \"beta\", \"gamma\", \"bx\", \"by\", \"bz\"]. Each row in df is an iteration of optimization with the updated values for that timestep.",
    "crumbs": [
      "api",
      "visualization"
    ]
  },
  {
    "objectID": "api/visualization.html#d-visualization-1",
    "href": "api/visualization.html#d-visualization-1",
    "title": "visualization",
    "section": "3D Visualization",
    "text": "3D Visualization\nUses pyvista and trame to interactively visualize DRR geometry in 3D.\n\nsource\n\ndrr_to_mesh\n\n drr_to_mesh (subject:torchio.data.subject.Subject, method:str,\n              threshold:float=300, verbose:bool=True)\n\n*Convert the CT in a DRR object into a mesh.\nIf using method==\"surface_nets\", ensure you have pyvista&gt;=0.43 and vtk&gt;=9.3 installed.\nThe mesh processing steps are:\n\nKeep only largest connected components\nSmooth\nDecimate (if method==\"marching_cubes\")\nFill any holes\nClean (remove any redundant vertices/edges)*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubject\nSubject\n\ntorchio.Subject with a volume attribute\n\n\nmethod\nstr\n\nEither surface_nets or marching_cubes\n\n\nthreshold\nfloat\n300\nMin value for marching cubes (Hounsfield units)\n\n\nverbose\nbool\nTrue\nDisplay progress bars for mesh processing steps\n\n\n\n\nsource\n\n\nlabelmap_to_mesh\n\n labelmap_to_mesh (subject:torchio.data.subject.Subject,\n                   verbose:bool=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubject\nSubject\n\ntorchio.Subject with a mask attribute\n\n\nverbose\nbool\nTrue\nDisplay progress bars for mesh processing steps\n\n\n\n\nsource\n\n\nimg_to_mesh\n\n img_to_mesh (drr:diffdrr.drr.DRR, pose:diffdrr.pose.RigidTransform,\n              calibration:diffdrr.pose.RigidTransform=None, **kwargs)\n\nFor a given pose (not batched), turn the camera and detector into a mesh. Additionally, render the DRR for the pose. Convert into a texture that can be applied to the detector mesh.",
    "crumbs": [
      "api",
      "visualization"
    ]
  },
  {
    "objectID": "api/pose.html",
    "href": "api/pose.html",
    "title": "pose",
    "section": "",
    "text": "We represent rigid transforms as \\(4 \\times 4\\) matrices\n\\[\\begin{equation}\n\\begin{bmatrix}\n    \\mathbf R & \\mathbf R \\mathbf t \\\\\n    \\mathbf 0 & 1\n\\end{bmatrix}\n\\in \\mathbf{SE}(3) \\,,\n\\end{equation}\\]\nwhere \\(\\mathbf R \\in \\mathbf{SO}(3)\\) is a rotation matrix and \\(\\mathbf t\\in \\mathbb R^3\\) represents a translation.\n\n\n\n\n\n\nTip\n\n\n\nIn this parameterization, \\(\\mathbf R \\mathbf t\\) represents the position of the C-arm source in world coordinates and \\(\\mathbf R\\) represents the orientation of the C-arm.\n\n\nNote that since rotation matrices are orthogonal (\\(\\mathbf R \\mathbf R^T = \\mathbf R^T \\mathbf R = \\mathbf I\\)), we have a simple closed-form equation for the inverse: \\[\\begin{equation}\n\\begin{bmatrix}\n    \\mathbf R & \\mathbf R \\mathbf t \\\\\n    \\mathbf 0 & 1\n\\end{bmatrix}^{-1} =\n\\begin{bmatrix}\n    \\mathbf R^T & -\\mathbf t \\\\\n    \\mathbf 0 & 1\n\\end{bmatrix} \\,.\n\\end{equation}\\]\n\nsource\n\n\n\n RigidTransform (matrix)\n\nApplies rigid transforms in SE(3) to point clouds. Can handle batched rigid transforms, composition of transforms, closed-form inversion, and conversions to various representations of SE(3).",
    "crumbs": [
      "api",
      "pose"
    ]
  },
  {
    "objectID": "api/pose.html#rigid-transformations",
    "href": "api/pose.html#rigid-transformations",
    "title": "pose",
    "section": "",
    "text": "We represent rigid transforms as \\(4 \\times 4\\) matrices\n\\[\\begin{equation}\n\\begin{bmatrix}\n    \\mathbf R & \\mathbf R \\mathbf t \\\\\n    \\mathbf 0 & 1\n\\end{bmatrix}\n\\in \\mathbf{SE}(3) \\,,\n\\end{equation}\\]\nwhere \\(\\mathbf R \\in \\mathbf{SO}(3)\\) is a rotation matrix and \\(\\mathbf t\\in \\mathbb R^3\\) represents a translation.\n\n\n\n\n\n\nTip\n\n\n\nIn this parameterization, \\(\\mathbf R \\mathbf t\\) represents the position of the C-arm source in world coordinates and \\(\\mathbf R\\) represents the orientation of the C-arm.\n\n\nNote that since rotation matrices are orthogonal (\\(\\mathbf R \\mathbf R^T = \\mathbf R^T \\mathbf R = \\mathbf I\\)), we have a simple closed-form equation for the inverse: \\[\\begin{equation}\n\\begin{bmatrix}\n    \\mathbf R & \\mathbf R \\mathbf t \\\\\n    \\mathbf 0 & 1\n\\end{bmatrix}^{-1} =\n\\begin{bmatrix}\n    \\mathbf R^T & -\\mathbf t \\\\\n    \\mathbf 0 & 1\n\\end{bmatrix} \\,.\n\\end{equation}\\]\n\nsource\n\n\n\n RigidTransform (matrix)\n\nApplies rigid transforms in SE(3) to point clouds. Can handle batched rigid transforms, composition of transforms, closed-form inversion, and conversions to various representations of SE(3).",
    "crumbs": [
      "api",
      "pose"
    ]
  },
  {
    "objectID": "api/pose.html#se3-conversions",
    "href": "api/pose.html#se3-conversions",
    "title": "pose",
    "section": "SE(3) Conversions",
    "text": "SE(3) Conversions\n\nsource\n\nconvert\n\n convert (*args, parameterization, convention=None)",
    "crumbs": [
      "api",
      "pose"
    ]
  },
  {
    "objectID": "api/pose.html#d-rotation-parameterization",
    "href": "api/pose.html#d-rotation-parameterization",
    "title": "pose",
    "section": "9D rotation parameterization",
    "text": "9D rotation parameterization\nSVDO+ (Levinson et al., 2020) use the SVD to symetmetrically orthogonalize a matrix.\n\nsource\n\nmatrix_to_rotation_9d\n\n matrix_to_rotation_9d (matrix:torch.Tensor)\n\n\nsource\n\n\nrotation_9d_to_matrix\n\n rotation_9d_to_matrix (rotation:torch.Tensor)\n\nConvert a 9-vector to a symmetrically orthogonalized rotation matrix via SVD.\n\n\n10D rotation parameterizations\nImplementations to convert rotation_10d (Peretroukhin et al., 2021) and quaternion_adjugate (Hanson and Hanson, 2022) parameterizations of SO(3) to quaternions.\n\nsource\n\n\nquaternion_to_rotation_10d\n\n quaternion_to_rotation_10d (q:torch.Tensor)\n\n\nsource\n\n\nrotation_10d_to_quaternion\n\n rotation_10d_to_quaternion (rotation:torch.Tensor)\n\n*Convert a 10-vector into a symmetric matrix, whose eigenvector corresponding to the eigenvalue of minimum modulus is the resulting quaternion.\nSource: https://arxiv.org/abs/2006.01031*\n\nsource\n\n\nquaternion_to_quaternion_adjugate\n\n quaternion_to_quaternion_adjugate (q:torch.Tensor)\n\n\nsource\n\n\nquaternion_adjugate_to_quaternion\n\n quaternion_adjugate_to_quaternion (rotation:torch.Tensor)\n\n*Convert a 10-vector in the quaternion adjugate, a symmetric matrix whose eigenvector corresponding to the eigenvalue of maximum modulus is the (unnormalized) quaternion. Uses a fast method to solve for the eigenvector without explicity computing the eigendecomposition.\nSource: https://arxiv.org/abs/2205.09116*\n\n\nPyTorch3D conversions port\nPyTorch3D has many useful conversion functions for transforming between multiple parameterizations of \\(\\mathbf{SO}(3)\\) and \\(\\mathbf{SE}(3)\\). However, installing PyTorch3D can be annoying for users not on Linux. We include the required conversion functions for PyTorch3D below. The original LICENSE from PyTorch3D is also included:\nBSD License\n\nFor PyTorch3D software\n\nCopyright (c) Meta Platforms, Inc. and affiliates. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n * Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n * Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n * Neither the name Meta nor the names of its contributors may be used to\n   endorse or promote products derived from this software without specific\n   prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
    "crumbs": [
      "api",
      "pose"
    ]
  },
  {
    "objectID": "api/detector.html",
    "href": "api/detector.html",
    "title": "detector",
    "section": "",
    "text": "Tip\n\n\n\nThe Detector is usually initialized in the DRR module and shouldn‚Äôt need to be called directly.\n\n\n\nsource\n\nDetector\n\n Detector (sdd:float, height:int, width:int, delx:float, dely:float,\n           x0:float, y0:float, reorient:&lt;built-\n           inmethodtensoroftypeobjectat0x7f969197bc40&gt;,\n           n_subsample:int|None=None, reverse_x_axis:bool=False)\n\nConstruct a 6 DoF X-ray detector system. This model is based on a C-Arm.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsdd\nfloat\n\nSource-to-detector distance (i.e., focal length)\n\n\nheight\nint\n\nHeight of the X-ray detector\n\n\nwidth\nint\n\nWidth of the X-ray detector\n\n\ndelx\nfloat\n\nPixel spacing in the X-direction\n\n\ndely\nfloat\n\nPixel spacing in the Y-direction\n\n\nx0\nfloat\n\nPrincipal point X-offset\n\n\ny0\nfloat\n\nPrincipal point Y-offset\n\n\nreorient\ntorch.tensor\n\nFrame-of-reference change matrix\n\n\nn_subsample\nint | None\nNone\nNumber of target points to randomly sample\n\n\nreverse_x_axis\nbool\nFalse\nIf pose includes reflection (in E(3) not SE(3)), reverse x-axis\n\n\n\n\nsource\n\n\nDetector.forward\n\n Detector.forward (extrinsic:diffdrr.pose.RigidTransform,\n                   calibration:diffdrr.pose.RigidTransform)\n\nCreate source and target points for X-rays to trace through the volume.",
    "crumbs": [
      "api",
      "detector"
    ]
  }
]